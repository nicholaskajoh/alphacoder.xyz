<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>K Means | Alpha Coder</title>

    <link href="https://fonts.googleapis.com/css?family=Merriweather" rel="stylesheet">
    
    <style>body{margin:40px auto;max-width:650px;line-height:1.6;font-family:merriweather,serif;font-size:16px;color:#444;padding:0 10px}h1,h2,h3{line-height:1.2}div.header h1{padding-top:0;padding-bottom:8px;margin-bottom:24px;font-size:15px;font-weight:400;border-bottom:1px solid}.header-menu{float:right}.header-menu a{padding-left:5px}ul.pagination{list-style-type:none;text-align:center;padding:0}ul.pagination>li{padding:0 8px;display:inline-block}div.footer{border-top:1px solid;text-align:center}.footer-links a{padding:0 5px}img{max-width:100%;max-height:100%;display:block;margin-left:auto;margin-right:auto}a,a:visited,a:hover,a:active{color:#00e}.posts-list li:not(:last-child){padding-bottom:20px}.post-meta a:not(:last-child){padding-right:5px}blockquote{margin-top:10px;margin-bottom:10px;margin-left:50px;padding-left:15px;border-left:3px solid #ccc;background-color:#f9f9f9;font-style:italic}</style>
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-82728104-5', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

    

    

    <meta property="og:title" content="K Means" />
<meta property="og:description" content="We’ve been talking classification for a while now — from K Nearest Neighbors to Naive Bayes to Support Vector Machines. In this post, we’ll be looking at clustering using an algorithm called K Means. Let’s dive in&hellip;
The ML Chops series  Linear Regression K Nearest Neighbors Naive Bayes Support Vector Machine K Means (this article)  K Means is an unsupervised learning algorithm that tries to cluster data into a specified number of groups, K based on feature similarity." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://alphacoder.xyz/k-means/" />
<meta property="article:published_time" content="2018-05-01T08:44:05+01:00" />
<meta property="article:modified_time" content="2018-05-01T08:44:05+01:00" />

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="K Means"/>
<meta name="twitter:description" content="We’ve been talking classification for a while now — from K Nearest Neighbors to Naive Bayes to Support Vector Machines. In this post, we’ll be looking at clustering using an algorithm called K Means. Let’s dive in&hellip;
The ML Chops series  Linear Regression K Nearest Neighbors Naive Bayes Support Vector Machine K Means (this article)  K Means is an unsupervised learning algorithm that tries to cluster data into a specified number of groups, K based on feature similarity."/>

    <meta itemprop="name" content="K Means">
<meta itemprop="description" content="We’ve been talking classification for a while now — from K Nearest Neighbors to Naive Bayes to Support Vector Machines. In this post, we’ll be looking at clustering using an algorithm called K Means. Let’s dive in&hellip;
The ML Chops series  Linear Regression K Nearest Neighbors Naive Bayes Support Vector Machine K Means (this article)  K Means is an unsupervised learning algorithm that tries to cluster data into a specified number of groups, K based on feature similarity.">
<meta itemprop="datePublished" content="2018-05-01T08:44:05&#43;01:00" />
<meta itemprop="dateModified" content="2018-05-01T08:44:05&#43;01:00" />
<meta itemprop="wordCount" content="841">



<meta itemprop="keywords" content="ML Chops Series," />
</head>


<body>
<div class="header">
    <h1>
        <a href="/">Alpha Coder</a>
        <div class="header-menu">
            <a href="/blog/">Blog</a>
            <a href="/support/">Support</a>
            <a href="/courses/">Courses</a>
        </div>
    </h1>
</div>
<div id="content">

<header>
    <h1>K Means</h1>
    

<div class="post-meta">
    <time datetime="2018-05-01">May 1, 2018</time>
    | Tags:
    <a href="http://alphacoder.xyz/tag/ml-chops-series/">ML Chops Series</a>
</div>
</header>
<article>
    <p>We’ve been talking <strong>classification</strong> for a while now — from <em>K Nearest Neighbors</em> to <em>Naive Bayes</em> to <em>Support Vector Machines</em>. In this post, we’ll be looking at <strong>clustering</strong> using an algorithm called <strong><em>K Means</em></strong>. Let’s dive in&hellip;</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*C2uxvEivKa4FQYimkl6OJA.jpeg" alt=""></p>
<h1 id="the-ml-chops-series">The ML Chops series</h1>
<ul>
<li><a href="/linear-regression">Linear Regression</a></li>
<li><a href="/k-nearest-neighbors">K Nearest Neighbors</a></li>
<li><a href="/naive-bayes">Naive Bayes</a></li>
<li><a href="/support-vector-machine">Support Vector Machine</a></li>
<li>K Means (this article)</li>
</ul>
<p>K Means is an unsupervised learning algorithm that tries to cluster data into a specified number of groups, <em>K</em> based on feature similarity. The algorithm works iteratively to assign each data point to one of <em>K</em> groups. Each group is identified by a centroid (the center point of the group) and data points are grouped with the centroid they’re closest to. The centroids are determined during training through optimization.</p>
<p><strong><em>NB:</em></strong> <em>Unsupervised learning involves training an algorithm with unclassified or unlabeled data. It’s up to the algorithm to figure out a pattern in the data and, in the case of clustering, group similar data points together.</em></p>
<h3 id="how-itworks">How it works</h3>
<ol>
<li>First, we select the number of groups/clusters, K we want. To figure out what value of K to use, you can visualize the data to see how many groups it can be separated into. This is not always possible because the data can be in more than 3 dimensions. In such a case, you need to have an expectation from your data. Maybe you’re trying to separate good oranges from bad ones. In this case, K = 2.</li>
<li>Next, we create K centroids. The easiest way to do this is to use the first K feature sets from our data or just do a random selection.</li>
<li>These centroids are not optimized. The training process involves moving the centroids until they’re at the center of each group, at which point we can quite correctly determine what group a given data point should be in (the centroid to which it’s closest). With our starting centroids, we iterate through the data grouping each point to the centroid it’s closest to. Afterwards, we change the centroids by taking the <strong>mean</strong> of all the data points in each group.</li>
<li>This process continues repetitively until we notice the centroids are not changing much or at all. This means we’re optimized!</li>
</ol>
<h3 id="code">Code</h3>
<p>First things first, the data</p>
<pre><code>import numpy as np

data = np.array([  
    [4, 3], [0, 0], [2, 4], [3, 4], [5, 4], [-2, 1], [-3, 0], [-3, -3], [8, 12], [11, 11], [9, 10]  
])
</code></pre>
<p>Visually, the above data looks like this</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*qsdD6dP7zqBd-jQO2sg0pQ.png" alt=""></p>
<p>From the graph you can easily see the data can be grouped into 3 clusters. Thus <code>K = 3</code>.</p>
<pre><code>K = 3  
tol = 0.001  
max_iter = 25
</code></pre>
<p>Notice the variables <code>tol</code> and <code>max_ter</code>. <code>tol</code> stands for tolerance and represents a percentage (0.001%). As we said earlier, we know we’re optimized when there’s little or no change in the centroids. If the change is greater than 0.001%, we tolerate and keep iterating.</p>
<p>But we can’t keep iterating forever/for too long in the case where we can’t optimize further and the change remains greater than 0.001. So we set <code>max_iter</code>, the maximum number of iterations we’re willing to do.</p>
<p>Next, let’s create the centroids. Their initial values would be the first K data points.</p>
<pre><code>centroids = {}  
for i in range(K):  
    centroids[i] = data[i]
</code></pre>
<p>Now unto the meat of the algorithm which is the optimization</p>
<pre><code>groups = {}  
for i in range(max_iter):  
    # step 1  
    for j in range(K):  
        groups[j] = []

    # step 2  
    for feature_set in data:  
        # step 2.1  
        distances = [np.linalg.norm(feature_set - centroids[centroid_key]) for centroid_key in centroids]

        # step 2.2  
        group = distances.index(min(distances))  
        groups[group].append(feature_set)

    # step 3  
    old_centroids = dict(centroids)

    # step 4  
    for j in range(K):  
        centroids[j] = np.average(groups[j], axis=0)

    # step 5  
    optimized = True  
    for centroid_key in centroids:  
        old_centroid = old_centroids[centroid_key]  
        new_centroid = centroids[centroid_key]  
        a = np.array(new_centroid - old_centroid)  
        b = np.array(old_centroid)  
        change = np.divide(a, b, out=np.zeros_like(a), where=b!=0)

        if abs(np.sum(change * 100.0)) &gt; tol:  
            optimized = False  
            break  
    if optimized:  
        break
</code></pre>
<p>Let break things down step by step…</p>
<h4 id="step-1">Step 1</h4>
<p>Initialize the <code>groups</code> dictionary with K empty arrays. This ensures <code>groups</code> is empty at the start of each iteration.</p>
<h4 id="step-21">Step 2.1</h4>
<p>Iterate through the data, calculating the euclidean distance of a given feature set from all the centroids.</p>
<h4 id="step-22">Step 2.2</h4>
<p>The feature set is added to the group of the centroid it’s closest to.</p>
<h4 id="step-3">Step 3</h4>
<p>Store the current centroids in <code>old_centroids</code>. The centroids are going to change soon. But we need the old centroids to see how much has changed since the last iteration.</p>
<h4 id="step-4">Step 4</h4>
<p>Calculate the new centroids by taking the average of all the feature sets in each centroid group.</p>
<h4 id="step-5">Step 5</h4>
<p>Check if the change in any of the centroids is greater than 0.001%. If yes, then we’re not yet optimized.</p>
<p>Here’s the full code for your perusal. It includes some lines of matplotlib code to visualize the data.</p>
<!-- raw HTML omitted -->
<p><img src="https://cdn-images-1.medium.com/max/800/1*xOImz1NtPQd8NrG5Y0AHWQ.png" alt=""></p>
<p>Check out the ML Chops repo for a class-based implementation and an example with real world data: <a href="https://github.com/nicholaskajoh/ML_Chops/tree/master/k-means">https://github.com/nicholaskajoh/ML_Chops/tree/master/k-means</a>.</p>
<p>If you have any questions, concerns or suggestions, don’t hesitate to comment! 👍</p>

</article>



<p style="font-style: italic;">Need help getting something to work in your project? Try <a href="/support/">Alpha Coder Support</a>!</p>


<form
  style="border: 1px solid #ccc; padding: 3px; text-align: center; border-radius: 2px; margin: 15px 0;"
  action="https://tinyletter.com/nicholaskajoh"
  method="post"
  target="popupwindow" onsubmit="window.open('https://tinyletter.com/nicholaskajoh', 'popupwindow', 'scrollbars=yes,width=800,height=600');return true">
    <div style="padding: 5px;">
      <h2>Subscribe to the Alpha Coder Newsletter!</h2>

      <p>Get timely updates on new articles, courses and more from Nicholas Kajoh. Unsubscribe anytime.</p>
      
      <input
        type="text"
        style="width: 150px; height: 30px; padding-left: 5px; border: 1px solid #ccc; border-radius: 2px;"
        name="email"
        placeholder="night.king@westeros.org">
      <input type="hidden" value="1" name="embed">
      <input
        type="submit"
        style="height: 34px; color: #fff; background-color: #00e; border: 1px solid #00e; border-radius: 2px;"
        value="Subscribe">
      
      <p>
        Enjoy the content on Alpha Coder? Please <a href="http://buymeacoff.ee/nicholaskajoh" target="_blank">buy me a coffee</a>. 😊
      </p>
    </div>
</form>




  
  
    <p>
      Previous post: <a href="http://alphacoder.xyz/image-upload-with-django-and-cloudinary/"><strong>Image upload with Django and Cloudinary</strong></a>
    </p>
  

  
    <p>
      Next post: <a href="http://alphacoder.xyz/scale-an-app-horizontally-using-a-load-balancer/"><strong>How to scale an app horizontally using a load balancer</strong></a>
    </p>
  
  



  <br><div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "alphacoderxyz" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>



    </div>
<div class="footer">
    
    
    <div class="footer-links">
        <a href="http://twitter.com/nicholaskajoh">Twitter</a>
        <a href="https://github.com/nicholaskajoh">GitHub</a>
        <a href="http://buymeacoff.ee/nicholaskajoh">Buy me a coffee</a>
        <a href="/blog/index.xml">RSS</a>
    </div>
    

    
    
    <div class="copyright">© Nicholas Kajoh</div>
    
</div>


<script src="https://unpkg.com/medium-zoom@1.0.4/dist/medium-zoom.min.js"></script>
<script>
  mediumZoom(document.querySelectorAll('img'));
</script></body>

</html>