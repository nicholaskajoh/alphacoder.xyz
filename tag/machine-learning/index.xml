<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Alpha Coder</title>
    <link>https://alphacoder.xyz/tag/machine-learning/</link>
    <description>Recent content in Machine Learning on Alpha Coder</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright 2017 to ∞. Nicholas Kajoh. All rights reserved.</copyright>
    <lastBuildDate>Fri, 12 Jul 2019 00:30:43 +0000</lastBuildDate><atom:link href="https://alphacoder.xyz/tag/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>I built a video-based vehicle counting system—here&#39;s how</title>
      <link>https://alphacoder.xyz/vehicle-counting/</link>
      <pubDate>Fri, 12 Jul 2019 00:30:43 +0000</pubDate>
      
      <guid>https://alphacoder.xyz/vehicle-counting/</guid>
      <description>I worked on a video-based vehicle counting system (VCS) for my final year (BSc) project. I shared a demo on Twitter that went semi-viral!
In this article, I&amp;rsquo;ll explain why and take you through how I built it, discussing how it works, how I learned the libraries used, the components of the system, the algorithms and models I experimented with and the results obtained. Let&amp;rsquo;s get started!
TL;DR: I built a video-based vehicle counting system using Python/OpenCV.</description>
    </item>
    
    <item>
      <title>Machine Learning explained</title>
      <link>https://alphacoder.xyz/machine-learning-explained/</link>
      <pubDate>Sat, 18 May 2019 15:22:12 +0000</pubDate>
      
      <guid>https://alphacoder.xyz/machine-learning-explained/</guid>
      <description>Over the past couple of weeks, I got to interact with quite a number of people who wanted to build AI (Artificial Intelligience) projects using Machine Learning (ML). There was one recurring problem I noticed in my discussions with them — they didn&amp;rsquo;t actually understand what Machine Learning is. And without understanding — at least on a high level — it&amp;rsquo;s nearly impossible to develop anything worth while. Except of course you intend to download AI projects off GitHub.</description>
    </item>
    
    <item>
      <title>K Means</title>
      <link>https://alphacoder.xyz/k-means/</link>
      <pubDate>Tue, 01 May 2018 08:44:05 +0100</pubDate>
      
      <guid>https://alphacoder.xyz/k-means/</guid>
      <description>We’ve been talking classification for a while now — from K Nearest Neighbors to Naive Bayes to Support Vector Machines. In this post, we’ll be looking at clustering using an algorithm called K Means. Let’s dive in&amp;hellip;
The ML Chops series Linear Regression K Nearest Neighbors Naive Bayes Support Vector Machine K Means (this article) K Means is an unsupervised learning algorithm that tries to cluster data into a specified number of groups, K based on feature similarity.</description>
    </item>
    
    <item>
      <title>Naive Bayes</title>
      <link>https://alphacoder.xyz/naive-bayes/</link>
      <pubDate>Tue, 26 Dec 2017 08:44:05 +0100</pubDate>
      
      <guid>https://alphacoder.xyz/naive-bayes/</guid>
      <description>Naive Bayes is a classifier just like K Nearest Neighbors. The Naive Bayes algorithm applies the popular Bayes Theorem (used to calculate conditional probability) given by the formula:
Bayes formula
Here’s a great explanation to read if you’ve not come across/don’t understand the theorem yet: https://betterexplained.com/articles/an-intuitive-and-short-explanation-of-bayes-theorem.
The ML Chops series Linear Regression K Nearest Neighbors Naive Bayes (this article) Support Vector Machine K Means Let’s consider the data we used in the last post on KNNs:</description>
    </item>
    
    <item>
      <title>Support Vector Machine</title>
      <link>https://alphacoder.xyz/support-vector-machine/</link>
      <pubDate>Tue, 26 Dec 2017 08:44:05 +0100</pubDate>
      
      <guid>https://alphacoder.xyz/support-vector-machine/</guid>
      <description>The Support Vector Machine (SVM) is a supervised learning model used for classification and regression. In this tutorial, we’ll be using it for classification.
The ML Chops series Linear Regression K Nearest Neighbors Naive Bayes Support Vector Machine (this article) K Means Created by Vladimir Vapnik in the 1960s, the SVM is one of most popular machine learning classifiers. Given a set of training samples, each marked as belonging to one or the other of two categories, the goal of SVM is to find the best splitting boundary between the data.</description>
    </item>
    
    <item>
      <title>K Nearest Neighbors</title>
      <link>https://alphacoder.xyz/k-nearest-neighbors/</link>
      <pubDate>Sat, 09 Dec 2017 08:44:05 +0100</pubDate>
      
      <guid>https://alphacoder.xyz/k-nearest-neighbors/</guid>
      <description>K Nearest Neighbors (KNN) is a Machine Learning algorithm for classification — a classifier as the experts would call it.
Classification is a very fundamental and important activity we perform as humans. We’ve grouped animals, plants, stars, humans, music etc to help us understand them and their relationships better, among other things. Often, we need to classify a thing as part of one of several groups. This crucial activity gets boring to do though.</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://alphacoder.xyz/linear-regression/</link>
      <pubDate>Mon, 04 Dec 2017 08:44:05 +0100</pubDate>
      
      <guid>https://alphacoder.xyz/linear-regression/</guid>
      <description>What better way to learn than to do? I decided to implement from scratch some Machine Learning algorithms I’m learning as a way to better understand and internalize them. The algorithms include Linear Regression, K Nearest Neighbors, Support Vector Machine, Naive Bayes, K Means and Neural Networks.
ML Chops is a series meant to explain the inner workings of these algorithms so you can get a pretty good grasp of how they work as well as know how to implement them yourself.</description>
    </item>
    
  </channel>
</rss>
