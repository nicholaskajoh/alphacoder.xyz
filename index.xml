<?xml-stylesheet href="/rss.xsl" type="text/xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>I code, therefore I am on Alpha Coder</title>
    <link>https://alphacoder.xyz/</link>
    <description>Recent content in I code, therefore I am on Alpha Coder</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright 2017 to ∞. Nicholas Kajoh. All rights reserved.</copyright>
    <lastBuildDate>Sun, 28 Aug 2022 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://alphacoder.xyz/index.xml" rel="self" type="application/rss+xml" />
    
    
    
        <item>
        <title>How to version REST APIs</title>
        <link>https://alphacoder.xyz/rest-api-versioning/</link>
        <pubDate>Sun, 28 Aug 2022 00:00:00 +0000</pubDate>
        
        <guid>https://alphacoder.xyz/rest-api-versioning/</guid>
        <description>Alpha Coder https://alphacoder.xyz/rest-api-versioning/ -&lt;p&gt;Having worked at companies that sell API products in the last couple years, I’ve found myself contemplating—for hours on end in the shower—what the best way to version REST APIs is. You want simplicity and stability so that your API is easy for developers to integrate. But you also want to iterate on your product and add new features to improve your offering. Eventually, you’ll need to introduce breaking changes. Maybe you’re expanding the scope of an endpoint so you want to reorganize the parameters in the request payload, or a field in the response body needs to be removed for security or compliance reasons. To deal with these, you’ll need to introduce versioning in some form.&lt;/p&gt;
&lt;p&gt;There are several ways to do API versioning. If you’ve integrated any 3rd-party APIs, you’re probably familiar with a few of them. You could put the version number in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the URL i.e &lt;code&gt;https://api.alphacoder.xyz/v1/posts&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;the accept header i.e &lt;code&gt;Accept: application/vnd.alphacoder.blog-v1+json&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;a custom header i.e &lt;code&gt;Accept-Version: 1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Rather than focusing on where to accept version numbers or what format they should be in (whatever you go with is fine as long as it’s consistent), your priority should be building a comprehensive versioning system around your API—or at least planning for one.&lt;/p&gt;
&lt;p&gt;Your versioning system should:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Abstract versioning for new users:&lt;/strong&gt; By default, new users of your API should be on the latest version so there’s no need to bore them with versioning initially. A good way to do this is to peg them to the latest version when they sign up to use your API. They won’t have to worry about versions until that version gets deprecated/sun-set or they want a feature that is not available in the version they’re using.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Use resource-level versioning:&lt;/strong&gt; That is, each resource should have it’s own version as opposed to using one version number for your entire API. With the right tooling e.g compatibility matrices, this should not be much of a problem. And for users consuming resources that seldom change, they won’t have to do anything in a long while.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Span all user contact surfaces:&lt;/strong&gt; So not just your API where a user can specify what version they want to use. On the user dashboards, they should be able to see a log of what resource versions they’re consuming and perhaps a nice graph of the data as well. They should also be able to see and change their default versions. On the documentation, they should be shown content specific to their default versions if applicable.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Have a deprecation and sun-setting policy:&lt;/strong&gt; When you have a well-documented deprecation and sun-setting policy and you communicate it to your users in advance, it makes upgrades less of a hassle and keeps everyone mostly happy. It could be time-based e.g versions are supported for 2 years after release, deprecated in their 3rd year and then sun-set afterwards. If you don’t release a lot of new versions, you could instead support the last 3-5 versions versions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Track API version usage:&lt;/strong&gt; This will give you a good view of how your system is used so that you can make certain decisions such as extending the lifespan of a version or reaching out to specific users to encourage them to upgrade.&lt;/li&gt;
&lt;/ul&gt;
- https://alphacoder.xyz/rest-api-versioning/ - Copyright 2017 to ∞. Nicholas Kajoh. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>Web app scaling techniques</title>
        <link>https://alphacoder.xyz/web-app-scaling-techniques/</link>
        <pubDate>Sat, 27 Aug 2022 00:00:00 +0000</pubDate>
        
        <guid>https://alphacoder.xyz/web-app-scaling-techniques/</guid>
        <description>Alpha Coder https://alphacoder.xyz/web-app-scaling-techniques/ -&lt;p&gt;Several moons ago, I wrote an article about &lt;a href=&#34;https://alphacoder.xyz/database-scaling-techniques/&#34;&gt;techniques for scaling databases&lt;/a&gt;. Today, we’ll be looking at the app side of things. As with databases, you want to develop your application with scalability in mind so that when the time comes to increase capacity, the process is straightforward and seamless. Having a clear pathway for building a higher capacity and robust system also helps prevent over-engineering and premature optimization, so it’s important to have the steps and process for evolving your systems to handle large workloads.&lt;/p&gt;
&lt;p&gt;Like the database article, I’ve organized the techniques in a relatively increasing order of relevance as one’s web app load grows.&lt;/p&gt;
&lt;h1 id=&#34;get-a-bigger-server&#34;&gt;Get a bigger server&lt;/h1&gt;
&lt;p&gt;It’s “engineer nature” to want to to engineer things, so most of us forget we could just &lt;a href=&#34;https://alphacoder.xyz/scale-an-app-horizontally-using-a-load-balancer/&#34;&gt;scale vertically&lt;/a&gt; i.e provision more resources on the server we’re currently using. If you use a cloud hosting service, this is as easy as clicking a few buttons and may not involve any downtime.&lt;/p&gt;
&lt;p&gt;Hardware has become faster and cheaper over the years and many server infrastructure providers offer beefy machines that can handle a lot of load (for example, you can rent an EC2 instance with 128 vCPUs, 4 TB of RAM, 4 TB of SSD storage and up to 25 Gbps network speed on AWS). If high availability is not a major concern for you (i.e you can tolerate some little downtime every now and then), consider all your options on the vertical side before going horizontal.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/web-app-scaling-techniques/get-thiccc-server.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;split-your-app-into-functional-components&#34;&gt;Split your app into functional components&lt;/h1&gt;
&lt;p&gt;You can break your app into functional components and spin up a server for each one. This will allow you increase capacity while preventing a resource-intensive component of your system from disrupting other components. Also, there are managed services for some of these components (such as AWS S3 for file storage), so you could offload some stuff to third parties and save time needed to develop and optimize them.&lt;/p&gt;
&lt;p&gt;Some components you could break your app into include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;User interface&lt;/li&gt;
&lt;li&gt;API&lt;/li&gt;
&lt;li&gt;Database&lt;/li&gt;
&lt;li&gt;Cache&lt;/li&gt;
&lt;li&gt;File storage&lt;/li&gt;
&lt;li&gt;Queue&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/web-app-scaling-techniques/split-app-into-comps.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;When splitting up your app, it’s important to consider the latency implications. Ideally, all the components should be in the same local network or region.&lt;/p&gt;
&lt;h1 id=&#34;run-multiple-instances-of-your-app&#34;&gt;Run multiple instances of your app&lt;/h1&gt;
&lt;p&gt;Eventually, a single server might not be able to handle your load requirements. More so, you might want your app to be highly available i.e if one or more servers are unhealthy or crash, your app should continue to work. In these cases, you’d want to scale horizontally i.e running your app on multiple machines and putting them behind a load balancer. A load balancer can spread the load evenly among the machines and stop routing traffic to a machine that is unhealthy.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/web-app-scaling-techniques/load-balancing.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;You could even go a step further and make your load balancing layer highly available by running multiple load balancers and glueing them together with a software like &lt;a href=&#34;https://www.keepalived.org/&#34;&gt;keepalived&lt;/a&gt; which allows you automatically fail over if a server goes down.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/web-app-scaling-techniques/multiple-load-balancers.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As with splitting your app into functional components, you should factor in the latency implications of putting your app behind a load balancer. Of course, the load balancers and app servers should be in the same location. Also, in order to run multiple instances of your app smoothly, the app should ideally be &lt;a href=&#34;https://www.webopedia.com/definitions/stateless/&#34;&gt;stateless&lt;/a&gt;. So consider removing sessions, in-memory caching and the like.&lt;/p&gt;
&lt;h1 id=&#34;load-balance-using-dns&#34;&gt;Load balance using DNS&lt;/h1&gt;
&lt;p&gt;In addition to resolving domain names, a DNS can be used as a load balancer. You can create multiple A records and configure your authoritative DNS in a round robin fashion, or preferably use geolocation-based routing i.e users will be routed to the servers closest to them. On top of being able to handle more load, this setup allows you achieve global high availability (by running your app in multiple data centers across the globe), as well as enables you provide consistent and lower latencies in all the regions you operate.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/web-app-scaling-techniques/dns-load-balancing.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;use-anycast-networking&#34;&gt;Use anycast networking&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cloudflare.com/learning/cdn/glossary/anycast-network/&#34;&gt;Anycast networking&lt;/a&gt; lets you use the internet as your app’s load balancer by advertising a single IP from multiple locations or PoPs (Points of Presence). This means that users are routed to the servers closest to them (in terms of router hops). As with DNS load balancing, this facilitates global high availability and lower latencies but it can be &lt;a href=&#34;https://labs.ripe.net/author/samir_jafferali/build-your-own-anycast-network-in-nine-steps/&#34;&gt;difficult and time consuming to set up&lt;/a&gt;. Also, it’s possible to &lt;a href=&#34;https://engineering.linkedin.com/network-performance/tcp-over-ip-anycast-pipe-dream-or-reality&#34;&gt;combine the two for better results&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/web-app-scaling-techniques/anycast-networking.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
- https://alphacoder.xyz/web-app-scaling-techniques/ - Copyright 2017 to ∞. Nicholas Kajoh. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>The case for monorepos</title>
        <link>https://alphacoder.xyz/monorepo-use-case/</link>
        <pubDate>Mon, 04 Jul 2022 19:48:14 +0200</pubDate>
        
        <guid>https://alphacoder.xyz/monorepo-use-case/</guid>
        <description>Alpha Coder https://alphacoder.xyz/monorepo-use-case/ -&lt;p&gt;At a previous company, I worked as a Platform Engineer for the Packages team. A brand new team, our job was to abstract the building blocks of our systems into reusable components, and to create a pipeline for developing and releasing these components as packages for use by product engineers. There was a lot of duplication in our codebases and we were also slowly moving towards a microservices architecture, so our work was crucial in helping the engineering organization improve code quality and developer productivity.&lt;/p&gt;
&lt;p&gt;I think we did a great job with what we built given the size of the team, the amount of time we had and how much freedom we had to modify/change existing systems and processes, but it wasn’t without issues. Weeks and months after leaving the team, I kept thinking about the initial design and how it could be improved. I think I’ve arrived at something better. Of course, it’s nothing new and some may find the trade-offs costlier than the benefits, but it’s quite fascinating to me because I was bewildered by and opposed to the idea when I first learned about it. I’m talking about monorepos.&lt;/p&gt;
&lt;h1 id=&#34;what-we-built&#34;&gt;What we built&lt;/h1&gt;
&lt;p&gt;Our stack was mainly JavaScript-based tech (TypeScript, Node.js and friends), so we were developing and releasing NPM packages. We used &lt;a href=&#34;https://github.com/semantic-release/semantic-release&#34;&gt;Semantic Release&lt;/a&gt; to automate the releases (versioning, release notes generation and publishing). We built a self-serve system to enable developers create new packages and update existing ones on their own. Creating a package involved setting up a GitHub repository with certain configs which connected it to the release pipeline. For existing packages, all a developer had to do was open a PR. Once merged, the changes would be cut into a new release and available in our private registry for installation/use.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# First time, add private registry to the package manager.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;npm login --scope&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;@test-company --registry&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;https://registry.testcompany.com
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Then install internal package(s).&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;npm install @test-company/very-useful-package
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;the-challenges&#34;&gt;The challenges&lt;/h1&gt;
&lt;p&gt;The challenges we faced were a result of the mismatch of the solution we implemented and our customers’ (i.e product developers’) needs. Our solution is a better fit for open source development where clear project scope and stability is king, as opposed to a fast-paced agile environment which prioritizes velocity and adaptability. Some of the issues we were confronted with are:&lt;/p&gt;
&lt;h2 id=&#34;more-complex-dev-environment&#34;&gt;More complex dev environment&lt;/h2&gt;
&lt;p&gt;Development is complex enough when you have to setup multiple repos and spin up multiple apps. Adding packages to the mix complicates things even more. Now you have to setup even more repos, work across multiple codebases, and update dependencies to deliver a single feature or fix a single bug. I did not realize how bad it was until I considered duplicating code from a package just to avoid having to update it and doing the whole release process dance.&lt;/p&gt;
&lt;h2 id=&#34;long-review-cycles&#34;&gt;Long review cycles&lt;/h2&gt;
&lt;p&gt;Multiple repos means multiple PRs for a single task. Multiple PRs means longer review cycles. We found, unsurprisingly, that developers who like to move fast don’t appreciate this very much.&lt;/p&gt;
&lt;h2 id=&#34;versioning-hell&#34;&gt;Versioning hell&lt;/h2&gt;
&lt;p&gt;The worst of the challenges was dealing with versioning. If you wanted to introduce a breaking change, you had 3 options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Don’t introduce a breaking change. Make your change backwards compatible by exposing some specialized function for your use case. This of course will increase complexity and ultimately defeat the goal of having clean abstractions that can be reused.&lt;/li&gt;
&lt;li&gt;Release a new version and update all the dependent apps. This can be a lot of work and may be risky.&lt;/li&gt;
&lt;li&gt;Release a new version just for your changes. Now you have to maintain multiple versions of a single package which is a pain.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;building-something-better&#34;&gt;Building something better&lt;/h1&gt;
&lt;p&gt;The solution to these problems is a monorepo. How? Well, with all the code in one place, you don’t have to clone a new repo, or open multiple PRs, or deal with versioning (you’re always using the latest version). Large refactors to create better abstractions are also more tenable.&lt;/p&gt;
&lt;p&gt;Of course, even if I recognized this at the time, there’s no way I could have convinced the engineering org or even my team to squash dozens of repos into one. The benefits of doing so just don’t justify the work involved—and the risks. Maybe at some point in the future it will and a monorepo would make sense.&lt;/p&gt;
&lt;p&gt;The thing I’ve learned in thinking through these challenges is that context is key. It’s important to suss out the motivations for using a given system or tool before criticizing or adopting it. Often, until you’re in the problem space yourself, you never recognize or appreciate the reasons for going with certain approaches.&lt;/p&gt;
- https://alphacoder.xyz/monorepo-use-case/ - Copyright 2017 to ∞. Nicholas Kajoh. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>Database scaling techniques</title>
        <link>https://alphacoder.xyz/database-scaling-techniques/</link>
        <pubDate>Sun, 07 Jun 2020 11:46:24 +0000</pubDate>
        
        <guid>https://alphacoder.xyz/database-scaling-techniques/</guid>
        <description>Alpha Coder https://alphacoder.xyz/database-scaling-techniques/ -&lt;p&gt;Over the years, I&amp;rsquo;ve had an unusual interest in techniques for scaling databases to meet high demands in terms of performance and reliability. I&amp;rsquo;m not exactly a fan of database administration but I&amp;rsquo;ve always had the anxiety that a design decision I&amp;rsquo;m making now will come back to haunt me in future. I quickly learned that trying to setup a system that can handle, say, a million users when one has only a thousand is a waste of time and resources. However, I wanted to know the progression that will lead me to such a point so that I can plan with foresight.&lt;/p&gt;
&lt;p&gt;After lots of reading and a fair amount of practice on the job, I&amp;rsquo;ve learned some of the common techniques used to scale databases. I&amp;rsquo;ve organized them in a relatively increasing order of relevance as one&amp;rsquo;s database load grows.&lt;/p&gt;
&lt;h1 id=&#34;query-optimization&#34;&gt;Query optimization&lt;/h1&gt;
&lt;p&gt;This is the good old technique of finding poorly written queries or queries that could be improved upon and making them more efficient. To be able to write optimal queries, you need to have a good handle of the query language (e.g SQL), as well as the database engine (e.g MySQL or PostgreSQL). Simple tips like selecting specific fields as opposed to selecting all fields (&lt;code&gt;SELECT *&lt;/code&gt;) when not required can go a long way in giving your application the performance boost it needs.&lt;/p&gt;
&lt;p&gt;Visual inspection of queries might not be sufficient as some queries only break when confronted with large amounts of data or heavy traffic. Application Performance Monitoring (APM) tools can be instrumental in finding these bottlenecks as they allow you view the performance metrics of all your queries and figure out which ones are degrading your database.&lt;/p&gt;
&lt;h1 id=&#34;indexing&#34;&gt;Indexing&lt;/h1&gt;
&lt;p&gt;Indexing in simple terms means organizing data in a way that makes it faster to retrieve. Relational databases allow you create indexes on one of more columns in your tables. This can result in huge performance gains with little or no effort and as such is highly recommended as a way to optimize your database. For the best results, &lt;a href=&#34;https://www.dbta.com/Columns/DBA-Corner/Top-10-Steps-to-Building-Useful-Database-Indexes-100498.aspx&#34;&gt;indexes should be based on the kind of queries being run against a given database&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;denormalization&#34;&gt;Denormalization&lt;/h1&gt;
&lt;p&gt;I&amp;rsquo;ve discussed denormalization in reasonable detail &lt;a href=&#34;https://alphacoder.xyz/database-denormalization/&#34;&gt;in a previous article&lt;/a&gt; so I won&amp;rsquo;t say much more than the basic idea of what it entails. The goal of denormalization is to improve read performance by adding redundant copies of data to make for faster access using simpler queries. This is essentially indexing but done by a schema designer rather than a database engine, and it can be a great addition to indexes if carried out purposefully.&lt;/p&gt;
&lt;h1 id=&#34;primary-replica-architecture&#34;&gt;Primary-replica architecture&lt;/h1&gt;
&lt;p&gt;This involves &lt;a href=&#34;https://stackoverflow.com/a/11715598/6293466&#34;&gt;scaling a database horizontally&lt;/a&gt; by running two or more instances. One is designated the primary database and handles writes while the others are replica databases which handle reads. The database engine uses a replication protocol to keep all the instances in sync by copying the data from the primary database to the replicas. This setup comprises a database cluster. The pros of the architecture include traffic load balancing and automatic data backups which result in improved performance and reliability of a database.&lt;/p&gt;
&lt;h1 id=&#34;multi-cluster-database&#34;&gt;Multi-cluster database&lt;/h1&gt;
&lt;p&gt;A multi-cluster database takes the primary-replica architecture to the next level. In this technique, the database tables that make up an application are grouped by function and each group is designated a cluster of its own. For instance, all the tables concerned with billing might be put in one cluster while those concerned with a feature, say private messaging, might be put in another. Care must be taken while grouping tables to avoid a situation where one or more related tables have been split across database clusters, and expensive operations are required to obtain a piece of data needed in your application as a result.&lt;/p&gt;
&lt;h1 id=&#34;sharding&#34;&gt;Sharding&lt;/h1&gt;
&lt;p&gt;Sharding is the process of horizontally partitioning data into multiple databases. Instead of splitting a database by groups of tables as described earlier, a table&amp;rsquo;s rows are split across multiple instances. In order to save or retrieve a piece of data, certain &lt;a href=&#34;https://www.citusdata.com/blog/2017/08/28/five-data-models-for-sharding/&#34;&gt;models&lt;/a&gt;/&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/architecture/patterns/sharding#sharding-strategies&#34;&gt;algorithms&lt;/a&gt; are employed in determining the &lt;a href=&#34;https://en.wikipedia.org/wiki/Shard_(database_architecture)&#34;&gt;shard&lt;/a&gt; to use/where the data resides. This adds a fair amount of complexity to your database setup and your application as well. Operations that might otherwise be straightforward can become a challenge under this configuration. As such sharding is usually reserved for last and carried out by admins who have a solid grasp of the database engine they&amp;rsquo;re dealing with.&lt;/p&gt;
- https://alphacoder.xyz/database-scaling-techniques/ - Copyright 2017 to ∞. Nicholas Kajoh. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>Database denormalization</title>
        <link>https://alphacoder.xyz/database-denormalization/</link>
        <pubDate>Fri, 05 Jun 2020 13:46:02 +0000</pubDate>
        
        <guid>https://alphacoder.xyz/database-denormalization/</guid>
        <description>Alpha Coder https://alphacoder.xyz/database-denormalization/ -&lt;p&gt;Normalization is a vital part of database schema design. The goal is to structure a relational database so as to reduce redundancy and improve the integrity of data. Given this understanding, denormalization sounds rather counter-intuitive. Why would one want to make a database &amp;ldquo;less normalized&amp;rdquo;?&lt;/p&gt;
&lt;p&gt;Well, it turns out that normalization comes at a performance cost for DB read operations. This is just fine for regular applications. However, read-heavy apps start to break at scale under this setup due to resource-intensive SQL queries involving joins, subqueries and the like. Denormalization helps improve read performance by adding redundant copies of data to make for faster access using simpler queries.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s worth noting that &lt;em&gt;denormalized data&lt;/em&gt; is different from &lt;em&gt;unnormalized data&lt;/em&gt;. While the latter refers to data before normalization, the former is a transformation of normalized data.&lt;/p&gt;
&lt;p&gt;So how does one denormalize? The changes to be made in order to denormalize your data vary based on the type of application, existing database schema and optimization needs. However, the reasoning behind them are the same i.e you want to make data retrieval as fast as possible. Outlined below are a few tips to help you in this process:&lt;/p&gt;
&lt;h1 id=&#34;duplicate-fields-that-seldom-if-ever-change&#34;&gt;Duplicate fields that seldom if ever change&lt;/h1&gt;
&lt;p&gt;Say you operate a popular ecommerce website and your fulfilment staff are having a hard time viewing the orders because the API that retrieves this information is slow. You might take a look and realize that the bottleneck is your &lt;em&gt;orders&lt;/em&gt; table which has lots of relationships that need to be populated in order to retrieve a couple important fields. Some of these fields will seldom, if ever change (at least within the period in which an order must be fulfilled) so you proceed to denormalize them by creating new columns in the &lt;em&gt;orders&lt;/em&gt; table.&lt;/p&gt;
&lt;p&gt;For example, you might determine that the list of ordered products rarely changes. However, in order to get this list, you&amp;rsquo;ve had to use a join to pull the data from the &lt;em&gt;products&lt;/em&gt; table. By adding an &lt;em&gt;ordered_items&lt;/em&gt; column to the &lt;em&gt;orders&lt;/em&gt; table which will contain JSON arrays of product SKUs and order quantities, you may be able to reduce the latency of the API by a substantial amount. This, coupled with other such optimizations has the potential to deliver significant performance gains.&lt;/p&gt;
&lt;p&gt;The cost of these changes is little. If the list of ordered products does change (a rarity), all it takes is updating the &lt;em&gt;ordered_items&lt;/em&gt; column. While this will increase disk usage, it&amp;rsquo;s not a concern for most.&lt;/p&gt;
&lt;h1 id=&#34;duplicate-fields-with-high-read-to-write-ratios&#34;&gt;Duplicate fields with high read to write ratios&lt;/h1&gt;
&lt;p&gt;Taking our ecommerce website scenario forward, you might discover that your products display pages have become slower because they&amp;rsquo;re one of the most trafficked part of the site. In seeking ways to optimize these pages, you realize you can simplify the query that fetches products by denormalizing some of the data in products-related tables including &lt;em&gt;product_options&lt;/em&gt;, &lt;em&gt;categories&lt;/em&gt;, &lt;em&gt;product_media&lt;/em&gt; and &lt;em&gt;tags&lt;/em&gt;. By adding several new columns in the &lt;em&gt;products&lt;/em&gt; table so you don&amp;rsquo;t have to query other tables, you might be able to increase the load times of these slow pages.&lt;/p&gt;
&lt;p&gt;It turns out that you don&amp;rsquo;t update products very often (maybe once a week on average). However, hundreds of people view these products every second. It makes sense to denormalize in this case because your products data has a high read-write ratio.&lt;/p&gt;
&lt;h1 id=&#34;create-columns-or-tables-for-frequently-used-aggregates&#34;&gt;Create columns or tables for frequently used aggregates&lt;/h1&gt;
&lt;p&gt;Frequently used aggregates (for example the total amount made from orders this week or the number of times a coupon has been used) don&amp;rsquo;t have to be computed on the fly. You can pre-compute them and store the results in another table or a new column for faster retrieval. If these aggregates are likely to change in future, you can recompute them periodically or at the point when their inputs change.&lt;/p&gt;
&lt;p&gt;It goes without saying that you should NOT denormalize until you encounter data retrieval issues. Like with most software engineering problems, there are tradeoffs! Also, denormalization is &lt;a href=&#34;https://alphacoder.xyz/database-scaling-techniques&#34;&gt;not the only way to optimize/scale databases&lt;/a&gt;.&lt;/p&gt;
- https://alphacoder.xyz/database-denormalization/ - Copyright 2017 to ∞. Nicholas Kajoh. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>node_modules: The Node.js black hole</title>
        <link>https://alphacoder.xyz/node-modules/</link>
        <pubDate>Mon, 25 May 2020 23:33:11 +0000</pubDate>
        
        <guid>https://alphacoder.xyz/node-modules/</guid>
        <description>Alpha Coder https://alphacoder.xyz/node-modules/ -&lt;p&gt;As a Node.js developer, you know just how large (in terms of number of files and directory size) node_modules can be (you&amp;rsquo;ve probably aready seen &lt;a href=&#34;https://www.reddit.com/r/ProgrammerHumor/comments/6s0wov/heaviest_objects_in_the_universe/&#34;&gt;the memes&lt;/a&gt;). But have you ever asked WHY? I hadn&amp;rsquo;t, up until recently, after which I did some goofing around on the interwebs out of curiosity.&lt;/p&gt;
&lt;p&gt;Turns out it has to do with 2 things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How npm (the Node package manager) resolves dependencies.&lt;/li&gt;
&lt;li&gt;How the Node.js community likes to develop packages.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;dependency-resolution&#34;&gt;Dependency resolution&lt;/h1&gt;
&lt;p&gt;npm differs from other popular package managers such as pip and RubGems in terms of how it resolves dependencies. npm may download multiple versions of a package where as pip/RubyGems will attempt to find a single version that satisfies all its dependents.&lt;/p&gt;
&lt;p&gt;Say your app has 2 dependencies &lt;em&gt;Package A&lt;/em&gt; and &lt;em&gt;Package B&lt;/em&gt;, and &lt;em&gt;Package B&lt;/em&gt; depends on &lt;em&gt;Package C&lt;/em&gt;, and &lt;em&gt;Package A&lt;/em&gt; and &lt;em&gt;Package C&lt;/em&gt; depend on &lt;em&gt;Package D&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;npm &lt;em&gt;might&lt;/em&gt; resolve your dependencies like so i.e downloading 2 copies of &lt;em&gt;Package D&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/nde-mdls/npm-dep-graph.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;On the other hand, pip/RubyGems will download 1 version of &lt;em&gt;Package D&lt;/em&gt; which satisfies &lt;em&gt;Package A&lt;/em&gt; and &lt;em&gt;Package C&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/nde-mdls/pip-rubygems-dep-graph.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;At first glance, the pip/RubyGems strategy might seem like the better approach but each has its pros and cons.&lt;/p&gt;
&lt;p&gt;While the pip/RubyGems dependency management approach conserves space, it can lead to &amp;ldquo;dependency hell&amp;rdquo; i.e a situation where the package manager is unable to find a version of a package that satisfies all its dependents. In such a case, the developer will have to fix things manually. This may involve upgrading/downgrading one or more dependencies and/or eliminating them entirely. As you might imagine, this can be a pain. Doing the job of a package manager is not fun. And should you decide to downgrade a package, you might be opening up your app to vulnerabilities. As your app grows and you add more dependencies, you are more likely to face dependency hell.&lt;/p&gt;
&lt;p&gt;The npm approach solves the dependency hell problem. If &lt;em&gt;Package A&lt;/em&gt; requires v2 of &lt;em&gt;Package D&lt;/em&gt; and &lt;em&gt;Package C&lt;/em&gt; requires v5 of &lt;em&gt;Package D&lt;/em&gt;, npm will download both versions. This is neat, but it doesn&amp;rsquo;t come without its challenges. As you are already well aware, your app bundles become very large. But there&amp;rsquo;s another problem that could occur in this approach. If you have one or more packages that expose a dependency as part of their interface, you might encounter version conflicts. For instance, if you have the latest version of React as a dependency in your project and you also have a component library dependency that uses an old version of React, you&amp;rsquo;d most likely encounter compatibility issues that might not be easy to detect at first glance. With the pip/RubyGems approach, you&amp;rsquo;d catch the problem pretty much at the start while trying to install the dependencies. Fortunately though, npm has a solution for this: &lt;a href=&#34;https://nodejs.org/es/blog/npm/peer-dependencies/&#34;&gt;peer dependencies&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s worth mentioning that npm optimizes your dependency graph by employing deduplication i.e if &lt;em&gt;Package A&lt;/em&gt; and &lt;em&gt;Package C&lt;/em&gt; require v1 of &lt;em&gt;Package D&lt;/em&gt;, npm will only download one copy of &lt;em&gt;Package D&lt;/em&gt;. You can run &lt;code&gt;npm ls&lt;/code&gt; in your project&amp;rsquo;s root directory to view the &amp;ldquo;deduped&amp;rdquo; packages in your dependency graph. Check out &lt;a href=&#34;https://gist.github.com/nicholaskajoh/a4b068818b965b95f6eae3aa285e4fc3&#34;&gt;the deduplicated packages&lt;/a&gt; in &lt;a href=&#34;https://www.npmjs.com/package/knex&#34;&gt;Knex&lt;/a&gt;, the SQL query builder.&lt;/p&gt;
&lt;p&gt;Looking at these 2 dependency management strategies with the merits and demerits in mind, the npm approach seems like the better one to me, but I may be biased.&lt;/p&gt;
&lt;h1 id=&#34;small-packages&#34;&gt;Small packages&lt;/h1&gt;
&lt;p&gt;The Node.js community is &amp;ldquo;notorious&amp;rdquo; for building &lt;a href=&#34;https://www.npmjs.com/~sindresorhus&#34;&gt;very small packages&lt;/a&gt;. It&amp;rsquo;s also big on reusing as much code as possible. The end result is npm packages that can easily contain a dozen or more dependencies which in turn have their own dependencies. Take Knex for example. As at the time of writing this article, it has 17 direct dependencies — dev dependencies not included (&lt;code&gt;npm ls --only=prod --depth=0 | wc -l&lt;/code&gt;) — and a total of 257 dependencies (&lt;code&gt;npm ls --only=prod | wc -l&lt;/code&gt;). This means if you start and new Node project and run &lt;code&gt;npm install knex&lt;/code&gt;, you&amp;rsquo;d have 258 dependencies on your hands right off the bat!&lt;/p&gt;
&lt;p&gt;One could argue that this pattern of building small packages and using lots of dependencies is fueled by npm&amp;rsquo;s dependency management strategy. If npm used the pip/RubyGems approach, Node.js developers would be wary of having many dependencies for fear of dependency hell. Perhaps there&amp;rsquo;s just one reason for the Node.js black hole. Having explored it, it seems pretty reasonable to me!&lt;/p&gt;
- https://alphacoder.xyz/node-modules/ - Copyright 2017 to ∞. Nicholas Kajoh. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>What level of abstraction should I play on?</title>
        <link>https://alphacoder.xyz/levels-of-abstraction/</link>
        <pubDate>Sun, 12 Apr 2020 18:17:48 +0000</pubDate>
        
        <guid>https://alphacoder.xyz/levels-of-abstraction/</guid>
        <description>Alpha Coder https://alphacoder.xyz/levels-of-abstraction/ -&lt;p&gt;&amp;ldquo;I built it from scratch!&amp;rdquo;&lt;/p&gt;
&lt;p&gt;A major sentiment among some of the developer circles I interacted with and was part of in my earlier days of coding was that you were a &lt;del&gt;better&lt;/del&gt; &amp;ldquo;real&amp;rdquo; developer if you could build stuff &amp;ldquo;from scratch&amp;rdquo;. You&amp;rsquo;d often hear people scoff at building websites with WordPress, for example. &amp;ldquo;I built this site from scratch with raw HTML, CSS and JavaScript. No framework or CMS!&amp;rdquo; It was a thing of pride. It meant you really knew your stuff.&lt;/p&gt;
&lt;p&gt;I was a WordPress guy at the time so this didn&amp;rsquo;t really sit well with me. Was I a fake developer for using WordPress? And what does building from scratch even mean? Not using frameworks, boilerplates or CMSes? Writing assembly or machine code? Crafting electric circuits?&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m spending quite some time these days tinkering with hardware and learning the more low level computer stuff. I&amp;rsquo;m particularly interested in &lt;a href=&#34;https://eater.net&#34;&gt;Ben Eater&amp;rsquo;s work&lt;/a&gt;. A couple weeks ago, I was watching one of his YouTube videos titled &lt;a href=&#34;https://www.youtube.com/watch?v=LnzuMJLZRdU&#34;&gt;&amp;ldquo;Hello, world from scratch on a 6502 — Part 1&amp;rdquo;&lt;/a&gt;. I found the top comment interesting.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/abstrn/yt-comment.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;While it&amp;rsquo;s very funny (for me at least), I&amp;rsquo;m pointing it out because it&amp;rsquo;s underlay by two ideas relevant to this article.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Technically speaking, &lt;a href=&#34;https://www.youtube.com/watch?v=7s664NsLeFM&#34;&gt;you can&amp;rsquo;t build anything from scratch&lt;/a&gt;. You have to start from somewhere and then create abstractions to offset complexity as it increases.&lt;/li&gt;
&lt;li&gt;There are many many levels of abstraction that make it possible for us to interact with computers the way we do. Much more than we might realize.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As a developer with a never-ending thirst for knowledge, I&amp;rsquo;ve always asked myself, &amp;ldquo;what level of abstraction should I play on?&amp;rdquo; There are just so many abstraction layers that it is not practical to work across all these levels. Moreso, abstractions are primarily intended to remove the need for interacting with other abstractions.&lt;/p&gt;
&lt;p&gt;After years of experimentation in different levels of abstraction to figure out where to play on, I came up with two criteria for determining the best abstractions for me to use and how to decide whether to move up or down the stack.&lt;/p&gt;
&lt;h1 id=&#34;does-it-offer-the-most-business-value&#34;&gt;Does it offer the most business value?&lt;/h1&gt;
&lt;p&gt;Much as I&amp;rsquo;d love to build my next website in assembly or better still, by stitching a bunch of transistors together, I&amp;rsquo;ve yet to because for me, there&amp;rsquo;s very little business value in doing so. While important, these abstractions offer little or no benefit to someone like me who needs to move and iterate quickly in order to achieve their goals. Whereas, abstractions like web frameworks, CMSes and website builders might be of great benefit.&lt;/p&gt;
&lt;p&gt;I was big on WordPress in my freelancing days not because I particularly enjoyed wading through dozens of themes to find something satisfactory or hacking plugins to suit my needs. It was because it provided a lot of value to me and my clients — low setup and maintenance costs, out-of-box content management, ample technical support and so on. I&amp;rsquo;ve been using a website builder to develop a couple sites lately. It&amp;rsquo;s been a huge time saver! And given that my time is becoming more expensive by the day, it&amp;rsquo;s a really nice value-add.&lt;/p&gt;
&lt;p&gt;As a rule of thumb, I go for the highest possible level of abstraction that can get me the functionality I need. This might mean using a fully-managed SaaS product rather than building and maintaining a service in-house, or using tooling and technologies that prioritize velocity in order to ship features and fix bugs faster.&lt;/p&gt;
&lt;h1 id=&#34;is-the-abstraction-starting-to-leak&#34;&gt;Is the abstraction starting to leak?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/&#34;&gt;The law of leaky abstractions&lt;/a&gt; states &amp;ldquo;All non-trivial abstractions, to some degree, are leaky.&amp;rdquo; While the first criteria encourages opting for a high level of abstraction which makes it easier and faster to achieve an objective, this one advocates moving one or more levels lower when abstractions begin to leak.&lt;/p&gt;
&lt;p&gt;If you find yourself writing a lot of CSS to override other CSS, perhaps it is time to drop Bootstrap and build your own UI component library, or pick another library with less abstraction. If you&amp;rsquo;re spending a lot of time hacking a framework because its APIs can no longer give you the functionality you need, you should probably start migrating away from it and building your own abstractions using language-native APIs.&lt;/p&gt;
&lt;p&gt;An exemplification of the basic philosophy is this: &lt;em&gt;I don&amp;rsquo;t have to worry about the intricacies of paradigm X in language Y because there&amp;rsquo;s this nifty little library that exposes a streamlined API for achieving what I want. When my needs grow beyond what the library offers, I&amp;rsquo;ll begin to dig into the weeds of the paradigm. Right now, I&amp;rsquo;m more focused on delivering value with what I&amp;rsquo;m building than trying to understand every detail of some component I need to consume. When the abstraction begins to leak, I&amp;rsquo;ll cross that river.&lt;/em&gt;&lt;/p&gt;
- https://alphacoder.xyz/levels-of-abstraction/ - Copyright 2017 to ∞. Nicholas Kajoh. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>Authentication strategies in microservices architecture</title>
        <link>https://alphacoder.xyz/microservices-architecture-authentication/</link>
        <pubDate>Sun, 29 Sep 2019 17:37:55 +0000</pubDate>
        
        <guid>https://alphacoder.xyz/microservices-architecture-authentication/</guid>
        <description>Alpha Coder https://alphacoder.xyz/microservices-architecture-authentication/ -&lt;p&gt;When moving from monolith to microservices or considering microservices for a greenfield project, it&amp;rsquo;s important to evaluate the authentication strategies available to you to find the one most suitable for your system, as authentication is an integral part of how most applications are interacted with.&lt;/p&gt;
&lt;p&gt;In this article, I outline the most common auth strategies I&amp;rsquo;ve come across working with microservices, describing how they work and identifying their pros and cons.&lt;/p&gt;
&lt;h1 id=&#34;auth-service&#34;&gt;Auth service&lt;/h1&gt;
&lt;p&gt;In this strategy, a microservice is created for the purpose of authentication. This service is contacted by downstream services required to authenticate client requests before processing them.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/msvc-auth/auth-svc.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It&amp;rsquo;s easy to implement and reason about, especially when transitioning from a monolith. The auth service is essentially a utility other services reach out to for their authentication needs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It introduces a single point of failure. If the auth service is down, the whole application (or most of it) is down.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Increased latency. The time taken to make a request to and get a response from the auth service can hurt the average response time of a microservices application negatively.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;jwt-verification-per-service&#34;&gt;JWT verification per service&lt;/h1&gt;
&lt;p&gt;This approach is an extension of the previous strategy and is intended to reduce dependence on the auth service. Authentication primarily involves issuing and verifying tokens. JWT (&lt;a href=&#34;https://jwt.io&#34;&gt;JSON Web Tokens&lt;/a&gt;) can be used to verify tokens without having to hit a database or other persistent storage. This means each service can verify requests on their own. Token issuing is done in the auth service, while verification is handled in every service where it&amp;rsquo;s required. A client library is usually used to share this verification functionality with all the services that need to perform authentication.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/msvc-auth/jwt-verifier.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;There&amp;rsquo;s no single point of failure, auth-wise. The authentication service is only used for issuing tokens. All other services can handle token verification on their own.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It&amp;rsquo;s easy to create a client library for token verification which can be shared by all the services that require auth (assuming a single tech stack is used across board).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;JWT can&amp;rsquo;t be invalidated on-demand. While this is not a problem for some types of applications, it&amp;rsquo;s a deal breaker for others e.g an enterprise app where an admin might want to disable or delete a user or a finance app where a user might want to sign out from one or more devices remotely.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Making a change to the verification logic (e.g adding a new permission) requires updating all dependent services. This might be a Herculean task if a large number of services are involved.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;token-blacklist-cache&#34;&gt;Token blacklist cache&lt;/h1&gt;
&lt;p&gt;To compensate for the shortcomings of JWT, a cache can be used to store tokens that need to be invalidated. This means that every service must contact a shared cache to check if a token has been blacklisted after successfully verifying it, before proceeding to execute any auth-protected code.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/msvc-auth/token-blacklist-cache.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;JWTs can be invalidated on-demand through blacklisting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This strategy is not feasible in a system where each service has its own datastore.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If multiple services are allowed to write to the cache, they might erroneously modify token blacklist data, causing problems that might be difficult to detect and fix.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The shared cache introduces a single point of failure to the system.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;api-gateway-auth&#34;&gt;API gateway auth&lt;/h1&gt;
&lt;p&gt;Most, if not all, microservices applications use an API gateway. An API gateway is the entry point where client requests go to before being sent to the appropriate API/edge services. The main function of a gateway is routing requests but they can be used for other purposes such as analytics, rate-limiting, data transformation, logging, health monitoring, caching and as you might have already guessed, authentication.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/msvc-auth/api-gateway-auth.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Separation of concerns. Services can communicate freely with each other and do what they need to do as authentication has already been handled downstream by the gateway.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Switching to an API gateway&amp;rsquo;s auth system from an existing application&amp;rsquo;s auth implementation can be difficult and time-consuming.&lt;/li&gt;
&lt;/ul&gt;
- https://alphacoder.xyz/microservices-architecture-authentication/ - Copyright 2017 to ∞. Nicholas Kajoh. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>How to connect to a host&#39;s database from inside a Docker container</title>
        <link>https://alphacoder.xyz/connect-to-host-database-from-docker-container/</link>
        <pubDate>Sun, 01 Sep 2019 06:29:54 +0100</pubDate>
        
        <guid>https://alphacoder.xyz/connect-to-host-database-from-docker-container/</guid>
        <description>Alpha Coder https://alphacoder.xyz/connect-to-host-database-from-docker-container/ -&lt;p&gt;There are several ways to interact with a DB when developing using Docker:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Connect to an online DB instance.&lt;/li&gt;
&lt;li&gt;Connect to a local DB running in another container.&lt;/li&gt;
&lt;li&gt;Connect to a local DB running on a host machine (e.g your laptop).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first option can be easy and fast to setup i.e if you&amp;rsquo;re using a managed DB service. But these conveniences come at a monetary cost. Plus, you won&amp;rsquo;t be able to develop without an internet connection.&lt;/p&gt;
&lt;p&gt;While the second and third options are similar in the sense that the DB runs locally, using a DB running on the host might be a more convenient option if you have that already setup, or if you don&amp;rsquo;t want to have to mount a volume on the host in order to persist your data or run an additional container.&lt;/p&gt;
&lt;p&gt;Connecting to a host DB from a container via localhost doesn&amp;rsquo;t work because both container and host have their own localhosts. When you try to connect to localhost, it fails because no DB instance is running in the container&amp;rsquo;s localhost as you might imagine. You need to go outside the container by using your computer&amp;rsquo;s internal IP address.&lt;/p&gt;
&lt;p&gt;You can point to your host&amp;rsquo;s DB by setting the following environment variable in your container for instance.&lt;/p&gt;
&lt;p&gt;Docker for Mac/Windows:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;DB_HOST=host.docker.internal
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Linux:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;DB_HOST=$(ip route show default | awk &amp;#39;/default/ {print $3}&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;- https://alphacoder.xyz/connect-to-host-database-from-docker-container/ - Copyright 2017 to ∞. Nicholas Kajoh. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>Here&#39;s a quick Elasticsearch-Kibana setup via Docker for development</title>
        <link>https://alphacoder.xyz/elasticsearch-kibana-docker-development-setup/</link>
        <pubDate>Sun, 01 Sep 2019 06:18:43 +0100</pubDate>
        
        <guid>https://alphacoder.xyz/elasticsearch-kibana-docker-development-setup/</guid>
        <description>Alpha Coder https://alphacoder.xyz/elasticsearch-kibana-docker-development-setup/ -&lt;p&gt;Are you new to the Elastic stack or configuring a new machine and need an easy way to setup, and a single command to run Elasticsearch &amp;amp; Kibana? Here&amp;rsquo;s a quick Elasticsearch-Kibana setup using Docker for your dev environment.&lt;/p&gt;
&lt;h1 id=&#34;setup&#34;&gt;Setup&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/install/&#34;&gt;Install Docker for your OS&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Pull the Docker image for Elasticsearch: &lt;code&gt;docker pull docker.elastic.co/elasticsearch/elasticsearch:7.3.0&lt;/code&gt; (you can change &lt;strong&gt;7.3.0&lt;/strong&gt; to &lt;a href=&#34;https://www.docker.elastic.co/&#34;&gt;the latest or your preferred version&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Pull the Docker image for Kibana: &lt;code&gt;docker pull docker.elastic.co/kibana/kibana:7.3.0&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;run&#34;&gt;Run&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker container stop es_dev &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; docker container rm es_dev
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --name es_dev &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    -p 9200:9200 &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    -p 9300:9300 &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    -e &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;discovery.type=single-node&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    docker.elastic.co/elasticsearch/elasticsearch:7.3.0 &amp;amp;&lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --link es_dev:elasticsearch &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    -p 5601:5601 &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    docker.elastic.co/kibana/kibana:7.3.0 &amp;amp;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The first command stops the container named &lt;code&gt;es_dev&lt;/code&gt; (arbitrary) if one exists and removes it. The second command starts a single-node Elasticsearch cluster inside a docker container named &lt;code&gt;es_dev&lt;/code&gt;, and exposes it on ports 9200 and 9300. The third and last command starts Kibana on port 5601 and links it to the container (&lt;code&gt;es_dev&lt;/code&gt;) where the Elasticsearch cluster is running. The last two commands are run concurrently.&lt;/p&gt;
&lt;p&gt;You can add these three commands to a shell file (e.g &lt;em&gt;elastic.sh&lt;/em&gt;) in order to run a single command to start Elasticsearch and Kibana i.e &lt;code&gt;chmod +x elastic.sh &amp;amp;&amp;amp; ./elastic.sh&lt;/code&gt; the first time and &lt;code&gt;./elastic.sh&lt;/code&gt; subsequently.&lt;/p&gt;
&lt;h1 id=&#34;test&#34;&gt;Test&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Elasticsearch: &lt;code&gt;curl http://127.0.0.1:9200/_cat/health&lt;/code&gt; (you should have a status of &lt;strong&gt;green&lt;/strong&gt; if everything went well).&lt;/li&gt;
&lt;li&gt;Kibana: visit &lt;strong&gt;http://localhost:5601&lt;/strong&gt; (the Kibana dashboard should load up).&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;configure&#34;&gt;Configure&lt;/h1&gt;
&lt;p&gt;Elasticsearch and Kibana can be configured using YAML. You can create config files and point to them when starting the containers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Elasticsearch: add &lt;code&gt;-v path/to/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml&lt;/code&gt; to &lt;code&gt;docker run&lt;/code&gt; command.&lt;/li&gt;
&lt;li&gt;Kibana: add &lt;code&gt;-v path/to/kibana.yml:/usr/share/kibana/config/kibana.yml&lt;/code&gt; to &lt;code&gt;docker run&lt;/code&gt; command.&lt;/li&gt;
&lt;/ul&gt;
- https://alphacoder.xyz/elasticsearch-kibana-docker-development-setup/ - Copyright 2017 to ∞. Nicholas Kajoh. All rights reserved.</description>
        </item>
    
    
  </channel>
</rss> 