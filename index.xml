<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Software engineering with great finesse on Alpha Coder</title>
    <link>https://alphacoder.xyz/</link>
    <description>Recent content in Software engineering with great finesse on Alpha Coder</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright 2020 to ∞. Try Catch Finally Solutions Ltd. All rights reserved.</copyright>
    <lastBuildDate>Sun, 07 Jun 2020 11:46:24 +0000</lastBuildDate>
    
        <atom:link href="https://alphacoder.xyz/index.xml" rel="self" type="application/rss+xml" />
    
    
    
        <item>
        <title>Database scaling techniques</title>
        <link>https://alphacoder.xyz/database-scaling-techniques/</link>
        <pubDate>Sun, 07 Jun 2020 11:46:24 +0000</pubDate>
        
        <guid>https://alphacoder.xyz/database-scaling-techniques/</guid>
        <description>Alpha Coder https://alphacoder.xyz/database-scaling-techniques/ -&lt;p&gt;Over the years, I&amp;rsquo;ve had an unusual interest in techniques for scaling databases to meet high demands in terms of performance and reliability. I&amp;rsquo;m not exactly a fan of database administration but I&amp;rsquo;ve always had the anxiety that a design decision I&amp;rsquo;m making now will come back to haunt me in future. I quickly learned that trying to setup a system that can handle, say, a million users when one has only a thousand is a waste of time and resources. However, I wanted to know the progression that will lead me to such a point so that I can plan with foresight.&lt;/p&gt;
&lt;p&gt;After lots of reading and a fair amount of practice on the job, I&amp;rsquo;ve learned some of the common techniques used to scale databases. I&amp;rsquo;ve organized them in a relatively increasing order of relevance as one&amp;rsquo;s database load grows.&lt;/p&gt;
&lt;h1 id=&#34;query-optimization&#34;&gt;Query optimization&lt;/h1&gt;
&lt;p&gt;This is the good old technique of finding poorly written queries or queries that could be improved upon and making them more efficient. To be able to write optimal queries, you need to have a good handle of the query language (e.g SQL), as well as the database engine (e.g MySQL or PostgreSQL). Simple tips like selecting specific fields as opposed to selecting all fields (&lt;code&gt;SELECT *&lt;/code&gt;) when not required can go a long way in giving your application the performance boost it needs.&lt;/p&gt;
&lt;p&gt;Visual inspection of queries might not be sufficient as some queries only break when confronted with large amounts of data or heavy traffic. Application Performance Monitoring (APM) tools can be instrumental in finding these bottlenecks as they allow you view the performance metrics of all your queries and figure out which ones are degrading your database.&lt;/p&gt;
&lt;h1 id=&#34;indexing&#34;&gt;Indexing&lt;/h1&gt;
&lt;p&gt;Indexing in simple terms means organizing data in a way that makes it faster to retrieve. Relational databases allow you create indexes on one of more columns in your tables. This can result in huge performance gains with little or no effort and as such is highly recommended as a way to optimize your database. For the best results, &lt;a href=&#34;https://www.dbta.com/Columns/DBA-Corner/Top-10-Steps-to-Building-Useful-Database-Indexes-100498.aspx&#34;&gt;indexes should be based on the kind of queries being run against a given database&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;denormalization&#34;&gt;Denormalization&lt;/h1&gt;
&lt;p&gt;I&amp;rsquo;ve discussed denormalization in reasonable detail &lt;a href=&#34;https://alphacoder.xyz/database-denormalization/&#34;&gt;in a previous article&lt;/a&gt; so I won&amp;rsquo;t say much more than the basic idea of what it entails. The goal of denormalization is to improve read performance by adding redundant copies of data to make for faster access using simpler queries. This is essentially indexing but done by a schema designer rather than a database engine, and it can be a great addition to indexes if carried out purposefully.&lt;/p&gt;
&lt;h1 id=&#34;primary-replica-architecture&#34;&gt;Primary-replica architecture&lt;/h1&gt;
&lt;p&gt;This involves &lt;a href=&#34;https://stackoverflow.com/a/11715598/6293466&#34;&gt;scaling a database horizontally&lt;/a&gt; by running two or more instances. One is designated the primary database and handles writes while the others are replica databases which handle reads. The database engine uses a replication protocol to keep all the instances in sync by copying the data from the primary database to the replicas. This setup comprises a database cluster. The pros of the architecture include traffic load balancing and automatic data backups which result in improved performance and reliability of a database.&lt;/p&gt;
&lt;h1 id=&#34;multi-cluster-database&#34;&gt;Multi-cluster database&lt;/h1&gt;
&lt;p&gt;A multi-cluster database takes the primary-replica architecture to the next level. In this technique, the database tables that make up an application are grouped by function and each group is designated a cluster of its own. For instance, all the tables concerned with billing might be put in one cluster while those concerned with a feature, say private messaging, might be put in another. Care must be taken while grouping tables to avoid a situation where one or more related tables have been split across database clusters, and expensive operations are required to obtain a piece of data needed in your application as a result.&lt;/p&gt;
&lt;h1 id=&#34;sharding&#34;&gt;Sharding&lt;/h1&gt;
&lt;p&gt;Sharding is the process of horizontally partitioning data into multiple databases. Instead of splitting a database by groups of tables as described earlier, a table&amp;rsquo;s rows are split across multiple instances. In order to save or retrieve a piece of data, certain &lt;a href=&#34;https://www.citusdata.com/blog/2017/08/28/five-data-models-for-sharding/&#34;&gt;models&lt;/a&gt;/&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/architecture/patterns/sharding#sharding-strategies&#34;&gt;algorithms&lt;/a&gt; are employed in determining the &lt;a href=&#34;https://en.wikipedia.org/wiki/Shard_(database_architecture)&#34;&gt;shard&lt;/a&gt; to use/where the data resides. This adds a fair amount of complexity to your database setup and your application as well. Operations that might otherwise be straightforward can become a challenge under this configuration. As such sharding is usually reserved for last and carried out by admins who have a solid grasp of the database engine they&amp;rsquo;re dealing with.&lt;/p&gt;
- https://alphacoder.xyz/database-scaling-techniques/ - Copyright 2020 to ∞. Try Catch Finally Solutions Ltd. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>Database denormalization</title>
        <link>https://alphacoder.xyz/database-denormalization/</link>
        <pubDate>Fri, 05 Jun 2020 13:46:02 +0000</pubDate>
        
        <guid>https://alphacoder.xyz/database-denormalization/</guid>
        <description>Alpha Coder https://alphacoder.xyz/database-denormalization/ -&lt;p&gt;Normalization is a vital part of database schema design. The goal is to structure a relational database so as to reduce redundancy and improve the integrity of data. Given this understanding, denormalization sounds rather counter-intuitive. Why would one want to make a database &amp;ldquo;less normalized&amp;rdquo;?&lt;/p&gt;
&lt;p&gt;Well, it turns out that normalization comes at a performance cost for DB read operations. This is just fine for regular applications. However, read-heavy apps start to break at scale under this setup due to resource-intensive SQL queries involving joins, subqueries and the like. Denormalization helps improve read performance by adding redundant copies of data to make for faster access using simpler queries.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s worth noting that &lt;em&gt;denormalized data&lt;/em&gt; is different from &lt;em&gt;unnormalized data&lt;/em&gt;. While the latter refers to data before normalization, the former is a transformation of normalized data.&lt;/p&gt;
&lt;p&gt;So how does one denormalize? The changes to be made in order to denormalize your data vary based on the type of application, existing database schema and optimization needs. However, the reasoning behind them are the same i.e you want to make data retrieval as fast as possible. Outlined below are a few tips to help you in this process:&lt;/p&gt;
&lt;h1 id=&#34;duplicate-fields-that-seldom-if-ever-change&#34;&gt;Duplicate fields that seldom if ever change&lt;/h1&gt;
&lt;p&gt;Say you operate a popular ecommerce website and your fulfilment staff are having a hard time viewing the orders because the API that retrieves this information is slow. You might take a look and realize that the bottleneck is your &lt;em&gt;orders&lt;/em&gt; table which has lots of relationships that need to be populated in order to retrieve a couple important fields. Some of these fields will seldom, if ever change (at least within the period in which an order must be fulfilled) so you proceed to denormalize them by creating new columns in the &lt;em&gt;orders&lt;/em&gt; table.&lt;/p&gt;
&lt;p&gt;For example, you might determine that the list of ordered products rarely changes. However, in order to get this list, you&amp;rsquo;ve had to use a join to pull the data from the &lt;em&gt;products&lt;/em&gt; table. By adding an &lt;em&gt;ordered_items&lt;/em&gt; column to the &lt;em&gt;orders&lt;/em&gt; table which will contain JSON arrays of product SKUs and order quantities, you may be able to reduce the latency of the API by a substantial amount. This, coupled with other such optimizations has the potential to deliver significant performance gains.&lt;/p&gt;
&lt;p&gt;The cost of these changes is little. If the list of ordered products does change (a rarity), all it takes is updating the &lt;em&gt;ordered_items&lt;/em&gt; column. While this will increase disk usage, it&amp;rsquo;s not a concern for most.&lt;/p&gt;
&lt;h1 id=&#34;duplicate-fields-with-high-read-to-write-ratios&#34;&gt;Duplicate fields with high read to write ratios&lt;/h1&gt;
&lt;p&gt;Taking our ecommerce website scenario forward, you might discover that your products display pages have become slower because they&amp;rsquo;re one of the most trafficked part of the site. In seeking ways to optimize these pages, you realize you can simplify the query that fetches products by denormalizing some of the data in products-related tables including &lt;em&gt;product_options&lt;/em&gt;, &lt;em&gt;categories&lt;/em&gt;, &lt;em&gt;product_media&lt;/em&gt; and &lt;em&gt;tags&lt;/em&gt;. By adding several new columns in the &lt;em&gt;products&lt;/em&gt; table so you don&amp;rsquo;t have to query other tables, you might be able to increase the load times of these slow pages.&lt;/p&gt;
&lt;p&gt;It turns out that you don&amp;rsquo;t update products very often (maybe once a week on average). However, hundreds of people view these products every second. It makes sense to denormalize in this case because your products data has a high read-write ratio.&lt;/p&gt;
&lt;h1 id=&#34;create-columns-or-tables-for-frequently-used-aggregates&#34;&gt;Create columns or tables for frequently used aggregates&lt;/h1&gt;
&lt;p&gt;Frequently used aggregates (for example the total amount made from orders this week or the number of times a coupon has been used) don&amp;rsquo;t have to be computed on the fly. You can pre-compute them and store the results in another table or a new column for faster retrieval. If these aggregates are likely to change in future, you can recompute them periodically or at the point when their inputs change.&lt;/p&gt;
&lt;p&gt;It goes without saying that you should NOT denormalize until you encounter data retrieval issues. Like with most software engineering problems, there are tradeoffs! Also, denormalization is &lt;a href=&#34;https://alphacoder.xyz/database-scaling-techniques&#34;&gt;not the only way to optimize/scale databases&lt;/a&gt;.&lt;/p&gt;
- https://alphacoder.xyz/database-denormalization/ - Copyright 2020 to ∞. Try Catch Finally Solutions Ltd. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>node_modules: The Node.js black hole</title>
        <link>https://alphacoder.xyz/node-modules/</link>
        <pubDate>Mon, 25 May 2020 23:33:11 +0000</pubDate>
        
        <guid>https://alphacoder.xyz/node-modules/</guid>
        <description>Alpha Coder https://alphacoder.xyz/node-modules/ -&lt;p&gt;As a Node.js developer, you know just how large (in terms of number of files and directory size) node_modules can be (you&amp;rsquo;ve probably aready seen &lt;a href=&#34;https://www.reddit.com/r/ProgrammerHumor/comments/6s0wov/heaviest_objects_in_the_universe/&#34;&gt;the memes&lt;/a&gt;). But have you ever asked WHY? I hadn&amp;rsquo;t, up until recently, after which I did some goofing around on the interwebs out of curiosity.&lt;/p&gt;
&lt;p&gt;Turns out it has to do with 2 things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How npm (the Node package manager) resolves dependencies.&lt;/li&gt;
&lt;li&gt;How the Node.js community likes to develop packages.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;dependency-resolution&#34;&gt;Dependency resolution&lt;/h1&gt;
&lt;p&gt;npm differs from other popular package managers such as pip and RubGems in terms of how it resolves dependencies. npm may download multiple versions of a package where as pip/RubyGems will attempt to find a single version that satisfies all its dependents.&lt;/p&gt;
&lt;p&gt;Say your app has 2 dependencies &lt;em&gt;Package A&lt;/em&gt; and &lt;em&gt;Package B&lt;/em&gt;, and &lt;em&gt;Package B&lt;/em&gt; depends on &lt;em&gt;Package C&lt;/em&gt;, and &lt;em&gt;Package A&lt;/em&gt; and &lt;em&gt;Package C&lt;/em&gt; depend on &lt;em&gt;Package D&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;npm &lt;em&gt;might&lt;/em&gt; resolve your dependencies like so i.e downloading 2 copies of &lt;em&gt;Package D&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/nde-mdls/npm-dep-graph.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;On the other hand, pip/RubyGems will download 1 version of &lt;em&gt;Package D&lt;/em&gt; which satisfies &lt;em&gt;Package A&lt;/em&gt; and &lt;em&gt;Package C&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/nde-mdls/pip-rubygems-dep-graph.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;At first glance, the pip/RubyGems strategy might seem like the better approach but each has its pros and cons.&lt;/p&gt;
&lt;p&gt;While the pip/RubyGems dependency management approach conserves space, it can lead to &amp;ldquo;dependency hell&amp;rdquo; i.e a situation where the package manager is unable to find a version of a package that satisfies all its dependents. In such a case, the developer will have to fix things manually. This may involve upgrading/downgrading one or more dependencies and/or eliminating them entirely. As you might imagine, this can be a pain. Doing the job of a package manager is not fun. And should you decide to downgrade a package, you might be opening up your app to vulnerabilities. As your app grows and you add more dependencies, you are more likely to face dependency hell.&lt;/p&gt;
&lt;p&gt;The npm approach solves the dependency hell problem. If &lt;em&gt;Package A&lt;/em&gt; requires v2 of &lt;em&gt;Package D&lt;/em&gt; and &lt;em&gt;Package C&lt;/em&gt; requires v5 of &lt;em&gt;Package D&lt;/em&gt;, npm will download both versions. This is neat, but it doesn&amp;rsquo;t come without its challenges. As you are already well aware, your app bundles become very large. But there&amp;rsquo;s another problem that could occur in this approach. If you have one or more packages that expose a dependency as part of their interface, you might encounter version conflicts. For instance, if you have the latest version of React as a dependency in your project and you also have a component library dependency that uses an old version of React, you&amp;rsquo;d most likely encounter compatibility issues that might not be easy to detect at first glance. With the pip/RubyGems approach, you&amp;rsquo;d catch the problem pretty much at the start while trying to install the dependencies. Fortunately though, npm has a solution for this: &lt;a href=&#34;https://nodejs.org/es/blog/npm/peer-dependencies/&#34;&gt;peer dependencies&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s worth mentioning that npm optimizes your dependency graph by employing deduplication i.e if &lt;em&gt;Package A&lt;/em&gt; and &lt;em&gt;Package C&lt;/em&gt; require v1 of &lt;em&gt;Package D&lt;/em&gt;, npm will only download one copy of &lt;em&gt;Package D&lt;/em&gt;. You can run &lt;code&gt;npm ls&lt;/code&gt; in your project&amp;rsquo;s root directory to view the &amp;ldquo;deduped&amp;rdquo; packages in your dependency graph. Check out &lt;a href=&#34;https://gist.github.com/nicholaskajoh/a4b068818b965b95f6eae3aa285e4fc3&#34;&gt;the deduplicated packages&lt;/a&gt; in &lt;a href=&#34;https://www.npmjs.com/package/knex&#34;&gt;Knex&lt;/a&gt;, the SQL query builder.&lt;/p&gt;
&lt;p&gt;Looking at these 2 dependency management strategies with the merits and demerits in mind, the npm approach seems like the better one to me, but I may be biased.&lt;/p&gt;
&lt;h1 id=&#34;small-packages&#34;&gt;Small packages&lt;/h1&gt;
&lt;p&gt;The Node.js community is &amp;ldquo;notorious&amp;rdquo; for building &lt;a href=&#34;https://www.npmjs.com/~sindresorhus&#34;&gt;very small packages&lt;/a&gt;. It&amp;rsquo;s also big on reusing as much code as possible. The end result is npm packages that can easily contain a dozen or more dependencies which in turn have their own dependencies. Take Knex for example. As at the time of writing this article, it has 17 direct dependencies — dev dependencies not included (&lt;code&gt;npm ls --only=prod --depth=0 | wc -l&lt;/code&gt;) — and a total of 257 dependencies (&lt;code&gt;npm ls --only=prod | wc -l&lt;/code&gt;). This means if you start and new Node project and run &lt;code&gt;npm install knex&lt;/code&gt;, you&amp;rsquo;d have 258 dependencies on your hands right off the bat!&lt;/p&gt;
&lt;p&gt;One could argue that this pattern of building small packages and using lots of dependencies is fueled by npm&amp;rsquo;s dependency management strategy. If npm used the pip/RubyGems approach, Node.js developers would be wary of having many dependencies for fear of dependency hell. Perhaps there&amp;rsquo;s just one reason for the Node.js black hole. Having explored it, it seems pretty reasonable to me!&lt;/p&gt;
- https://alphacoder.xyz/node-modules/ - Copyright 2020 to ∞. Try Catch Finally Solutions Ltd. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>What level of abstraction should I play on?</title>
        <link>https://alphacoder.xyz/levels-of-abstraction/</link>
        <pubDate>Sun, 12 Apr 2020 18:17:48 +0000</pubDate>
        
        <guid>https://alphacoder.xyz/levels-of-abstraction/</guid>
        <description>Alpha Coder https://alphacoder.xyz/levels-of-abstraction/ -&lt;p&gt;&amp;ldquo;I built it from scratch!&amp;rdquo;&lt;/p&gt;
&lt;p&gt;A major sentiment among some of the developer circles I interacted with and was part of in my earlier days of coding was that you were a &lt;del&gt;better&lt;/del&gt; &amp;ldquo;real&amp;rdquo; developer if you could build stuff &amp;ldquo;from scratch&amp;rdquo;. You&amp;rsquo;d often hear people scoff at building websites with WordPress, for example. &amp;ldquo;I built this site from scratch with raw HTML, CSS and JavaScript. No framework or CMS!&amp;rdquo; It was a thing of pride. It meant you really knew your stuff.&lt;/p&gt;
&lt;p&gt;I was a WordPress guy at the time so this didn&amp;rsquo;t really sit well with me. Was I a fake developer for using WordPress? And what does building from scratch even mean? Not using frameworks, boilerplates or CMSes? Writing assembly or machine code? Crafting electric circuits?&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m spending quite some time these days tinkering with hardware and learning the more low level computer stuff. I&amp;rsquo;m particularly interested in &lt;a href=&#34;https://eater.net&#34;&gt;Ben Eater&amp;rsquo;s work&lt;/a&gt;. A couple weeks ago, I was watching one of his YouTube videos titled &lt;a href=&#34;https://www.youtube.com/watch?v=LnzuMJLZRdU&#34;&gt;&amp;ldquo;Hello, world from scratch on a 6502 — Part 1&amp;rdquo;&lt;/a&gt;. I found the top comment interesting.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/abstrn/yt-comment.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;While it&amp;rsquo;s very funny (for me at least), I&amp;rsquo;m pointing it out because it&amp;rsquo;s underlay by two ideas relevant to this article.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Technically speaking, &lt;a href=&#34;https://www.youtube.com/watch?v=7s664NsLeFM&#34;&gt;you can&amp;rsquo;t build anything from scratch&lt;/a&gt;. You have to start from somewhere and then create abstractions to offset complexity as it increases.&lt;/li&gt;
&lt;li&gt;There are many many levels of abstraction that make it possible for us to interact with computers the way we do. Much more than we might realize.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As a developer with a never-ending thirst for knowledge, I&amp;rsquo;ve always asked myself, &amp;ldquo;what level of abstraction should I play on?&amp;rdquo; There are just so many abstraction layers that it is not practical to work across all these levels. Moreso, abstractions are primarily intended to remove the need for interacting with other abstractions.&lt;/p&gt;
&lt;p&gt;After years of experimentation in different levels of abstraction to figure out where to play on, I came up with two criteria for determining the best abstractions for me to use and how to decide whether to move up or down the stack.&lt;/p&gt;
&lt;h1 id=&#34;does-it-offer-the-most-business-value&#34;&gt;Does it offer the most business value?&lt;/h1&gt;
&lt;p&gt;Much as I&amp;rsquo;d love to build my next website in assembly or better still, by stitching a bunch of transistors together, I&amp;rsquo;ve yet to because for me, there&amp;rsquo;s very little business value in doing so. While important, these abstractions offer little or no benefit to someone like me who needs to move and iterate quickly in order to achieve their goals. Whereas, abstractions like web frameworks, CMSes and website builders might be of great benefit.&lt;/p&gt;
&lt;p&gt;I was big on WordPress in my freelancing days not because I particularly enjoyed wading through dozens of themes to find something satisfactory or hacking plugins to suit my needs. It was because it provided a lot of value to me and my clients — low setup and maintenance costs, out-of-box content management, ample technical support and so on. I&amp;rsquo;ve been using a website builder to develop a couple sites lately. It&amp;rsquo;s been a huge time saver! And given that my time is becoming more expensive by the day, it&amp;rsquo;s a really nice value-add.&lt;/p&gt;
&lt;p&gt;As a rule of thumb, I go for the highest possible level of abstraction that can get me the functionality I need. This might mean using a fully-managed SaaS product rather than building and maintaining a service in-house, or using tooling and technologies that prioritize velocity in order to ship features and fix bugs faster.&lt;/p&gt;
&lt;h1 id=&#34;is-the-abstraction-starting-to-leak&#34;&gt;Is the abstraction starting to leak?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/&#34;&gt;The law of leaky abstractions&lt;/a&gt; states &amp;ldquo;All non-trivial abstractions, to some degree, are leaky.&amp;rdquo; While the first criteria encourages opting for a high level of abstraction which makes it easier and faster to achieve an objective, this one advocates moving one or more levels lower when abstractions begin to leak.&lt;/p&gt;
&lt;p&gt;If you find yourself writing a lot of CSS to override other CSS, perhaps it is time to drop Bootstrap and build your own UI component library, or pick another library with less abstraction. If you&amp;rsquo;re spending a lot of time hacking a framework because its APIs can no longer give you the functionality you need, you should probably start migrating away from it and building your own abstractions using language-native APIs.&lt;/p&gt;
&lt;p&gt;An exemplification of the basic philosophy is this: &lt;em&gt;I don&amp;rsquo;t have to worry about the intricacies of paradigm X in language Y because there&amp;rsquo;s this nifty little library that exposes a streamlined API for achieving what I want. When my needs grow beyond what the library offers, I&amp;rsquo;ll begin to dig into the weeds of the paradigm. Right now, I&amp;rsquo;m more focused on delivering value with what I&amp;rsquo;m building than trying to understand every detail of some component I need to consume. When the abstraction begins to leak, I&amp;rsquo;ll cross that river.&lt;/em&gt;&lt;/p&gt;
- https://alphacoder.xyz/levels-of-abstraction/ - Copyright 2020 to ∞. Try Catch Finally Solutions Ltd. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>Authentication strategies in microservices architecture</title>
        <link>https://alphacoder.xyz/microservices-architecture-authentication/</link>
        <pubDate>Sun, 29 Sep 2019 17:37:55 +0000</pubDate>
        
        <guid>https://alphacoder.xyz/microservices-architecture-authentication/</guid>
        <description>Alpha Coder https://alphacoder.xyz/microservices-architecture-authentication/ -&lt;p&gt;When moving from monolith to microservices or considering microservices for a greenfield project, it&amp;rsquo;s important to evaluate the authentication strategies available to you to find the one most suitable for your system, as authentication is an integral part of how most applications are interacted with.&lt;/p&gt;
&lt;p&gt;In this article, I outline the most common auth strategies I&amp;rsquo;ve come across working with microservices, describing how they work and identifying their pros and cons.&lt;/p&gt;
&lt;h1 id=&#34;auth-service&#34;&gt;Auth service&lt;/h1&gt;
&lt;p&gt;In this strategy, a microservice is created for the purpose of authentication. This service is contacted by downstream services required to authenticate client requests before processing them.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/msvc-auth/auth-svc.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It&amp;rsquo;s easy to implement and reason about, especially when transitioning from a monolith. The auth service is essentially a utility other services reach out to for their authentication needs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It introduces a single point of failure. If the auth service is down, the whole application (or most of it) is down.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Increased latency. The time taken to make a request to and get a response from the auth service can hurt the average response time of a microservices application negatively.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;jwt-verification-per-service&#34;&gt;JWT verification per service&lt;/h1&gt;
&lt;p&gt;This approach is an extension of the previous strategy and is intended to reduce dependence on the auth service. Authentication primarily involves issuing and verifying tokens. JWT (&lt;a href=&#34;https://jwt.io&#34;&gt;JSON Web Tokens&lt;/a&gt;) can be used to verify tokens without having to hit a database or other persistent storage. This means each service can verify requests on their own. Token issuing is done in the auth service, while verification is handled in every service where it&amp;rsquo;s required. A client library is usually used to share this verification functionality with all the services that need to perform authentication.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/msvc-auth/jwt-verifier.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;There&amp;rsquo;s no single point of failure, auth-wise. The authentication service is only used for issuing tokens. All other services can handle token verification on their own.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It&amp;rsquo;s easy to create a client library for token verification which can be shared by all the services that require auth (assuming a single tech stack is used across board).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;JWT can&amp;rsquo;t be invalidated on-demand. While this is not a problem for some types of applications, it&amp;rsquo;s a deal breaker for others e.g an enterprise app where an admin might want to disable or delete a user or a finance app where a user might want to sign out from one or more devices remotely.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Making a change to the verification logic (e.g adding a new permission) requires updating all dependent services. This might be a Herculean task if a large number of services are involved.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;token-blacklist-cache&#34;&gt;Token blacklist cache&lt;/h1&gt;
&lt;p&gt;To compensate for the shortcomings of JWT, a cache can be used to store tokens that need to be invalidated. This means that every service must contact a shared cache to check if a token has been blacklisted after successfully verifying it, before proceeding to execute any auth-protected code.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/msvc-auth/token-blacklist-cache.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;JWTs can be invalidated on-demand through blacklisting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This strategy is not feasible in a system where each service has its own datastore.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If multiple services are allowed to write to the cache, they might erroneously modify token blacklist data, causing problems that might be difficult to detect and fix.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The shared cache introduces a single point of failure to the system.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;api-gateway-auth&#34;&gt;API gateway auth&lt;/h1&gt;
&lt;p&gt;Most, if not all, microservices applications use an API gateway. An API gateway is the entry point where client requests go to before being sent to the appropriate API/edge services. The main function of a gateway is routing requests but they can be used for other purposes such as analytics, rate-limiting, data transformation, logging, health monitoring, caching and as you might have already guessed, authentication.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/msvc-auth/api-gateway-auth.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Separation of concerns. Services can communicate freely with each other and do what they need to do as authentication has already been handled downstream by the gateway.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Switching to an API gateway&amp;rsquo;s auth system from an existing application&amp;rsquo;s auth implementation can be difficult and time-consuming.&lt;/li&gt;
&lt;/ul&gt;
- https://alphacoder.xyz/microservices-architecture-authentication/ - Copyright 2020 to ∞. Try Catch Finally Solutions Ltd. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>How to connect to a host&#39;s database from inside a Docker container</title>
        <link>https://alphacoder.xyz/connect-to-host-database-from-docker-container/</link>
        <pubDate>Sun, 01 Sep 2019 06:29:54 +0100</pubDate>
        
        <guid>https://alphacoder.xyz/connect-to-host-database-from-docker-container/</guid>
        <description>Alpha Coder https://alphacoder.xyz/connect-to-host-database-from-docker-container/ -&lt;p&gt;There are several ways to interact with a DB when developing using Docker:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Connect to an online DB instance.&lt;/li&gt;
&lt;li&gt;Connect to a local DB running in another container.&lt;/li&gt;
&lt;li&gt;Connect to a local DB running on a host machine (e.g your laptop).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first option can be easy and fast to setup i.e if you&amp;rsquo;re using a managed DB service. But these conveniences come at a monetary cost. Plus, you won&amp;rsquo;t be able to develop without an internet connection.&lt;/p&gt;
&lt;p&gt;While the second and third options are similar in the sense that the DB runs locally, using a DB running on the host might be a more convenient option if you have that already setup, or if you don&amp;rsquo;t want to have to mount a volume on the host in order to persist your data or run an additional container.&lt;/p&gt;
&lt;p&gt;Connecting to a host DB from a container via localhost doesn&amp;rsquo;t work because both container and host have their own localhosts. When you try to connect to localhost, it fails because no DB instance is running in the container&amp;rsquo;s localhost as you might imagine. You need to go outside the container by using your computer&amp;rsquo;s internal IP address.&lt;/p&gt;
&lt;p&gt;You can point to your host&amp;rsquo;s DB by setting the following environment variable in your container for instance.&lt;/p&gt;
&lt;p&gt;Docker for Mac/Windows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;DB_HOST=host.docker.internal
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Linux:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;DB_HOST=$(ip route show default | awk &#39;/default/ {print $3}&#39;)
&lt;/code&gt;&lt;/pre&gt;- https://alphacoder.xyz/connect-to-host-database-from-docker-container/ - Copyright 2020 to ∞. Try Catch Finally Solutions Ltd. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>Here&#39;s a quick Elasticsearch-Kibana setup via Docker for development</title>
        <link>https://alphacoder.xyz/elasticsearch-kibana-docker-development-setup/</link>
        <pubDate>Sun, 01 Sep 2019 06:18:43 +0100</pubDate>
        
        <guid>https://alphacoder.xyz/elasticsearch-kibana-docker-development-setup/</guid>
        <description>Alpha Coder https://alphacoder.xyz/elasticsearch-kibana-docker-development-setup/ -&lt;p&gt;Are you new to the Elastic stack or configuring a new machine and need an easy way to setup, and a single command to run Elasticsearch &amp;amp; Kibana? Here&amp;rsquo;s a quick Elasticsearch-Kibana setup using Docker for your dev environment.&lt;/p&gt;
&lt;h1 id=&#34;setup&#34;&gt;Setup&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/install/&#34;&gt;Install Docker for your OS&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Pull the Docker image for Elasticsearch: &lt;code&gt;docker pull docker.elastic.co/elasticsearch/elasticsearch:7.3.0&lt;/code&gt; (you can change &lt;strong&gt;7.3.0&lt;/strong&gt; to &lt;a href=&#34;https://www.docker.elastic.co/&#34;&gt;the latest or your preferred version&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Pull the Docker image for Kibana: &lt;code&gt;docker pull docker.elastic.co/kibana/kibana:7.3.0&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;run&#34;&gt;Run&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;docker container stop es_dev &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; docker container rm es_dev
docker run &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --name es_dev &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    -p 9200:9200 &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    -p 9300:9300 &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    -e &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;discovery.type=single-node&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    docker.elastic.co/elasticsearch/elasticsearch:7.3.0 &amp;amp;&lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;
docker run &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --link es_dev:elasticsearch &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    -p 5601:5601 &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    docker.elastic.co/kibana/kibana:7.3.0 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The first command stops the container named &lt;code&gt;es_dev&lt;/code&gt; (arbitrary) if one exists and removes it. The second command starts a single-node Elasticsearch cluster inside a docker container named &lt;code&gt;es_dev&lt;/code&gt;, and exposes it on ports 9200 and 9300. The third and last command starts Kibana on port 5601 and links it to the container (&lt;code&gt;es_dev&lt;/code&gt;) where the Elasticsearch cluster is running. The last two commands are run concurrently.&lt;/p&gt;
&lt;p&gt;You can add these three commands to a shell file (e.g &lt;em&gt;elastic.sh&lt;/em&gt;) in order to run a single command to start Elasticsearch and Kibana i.e &lt;code&gt;chmod +x elastic.sh &amp;amp;&amp;amp; ./elastic.sh&lt;/code&gt; the first time and &lt;code&gt;./elastic.sh&lt;/code&gt; subsequently.&lt;/p&gt;
&lt;h1 id=&#34;test&#34;&gt;Test&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Elasticsearch: &lt;code&gt;curl http://127.0.0.1:9200/_cat/health&lt;/code&gt; (you should have a status of &lt;strong&gt;green&lt;/strong&gt; if everything went well).&lt;/li&gt;
&lt;li&gt;Kibana: visit &lt;strong&gt;http://localhost:5601&lt;/strong&gt; (the Kibana dashboard should load up).&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;configure&#34;&gt;Configure&lt;/h1&gt;
&lt;p&gt;Elasticsearch and Kibana can be configured using YAML. You can create config files and point to them when starting the containers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Elasticsearch: add &lt;code&gt;-v path/to/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml&lt;/code&gt; to &lt;code&gt;docker run&lt;/code&gt; command.&lt;/li&gt;
&lt;li&gt;Kibana: add &lt;code&gt;-v path/to/kibana.yml:/usr/share/kibana/config/kibana.yml&lt;/code&gt; to &lt;code&gt;docker run&lt;/code&gt; command.&lt;/li&gt;
&lt;/ul&gt;
- https://alphacoder.xyz/elasticsearch-kibana-docker-development-setup/ - Copyright 2020 to ∞. Try Catch Finally Solutions Ltd. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>I built a video-based vehicle counting system — here&#39;s how</title>
        <link>https://alphacoder.xyz/vehicle-counting/</link>
        <pubDate>Fri, 12 Jul 2019 00:30:43 +0000</pubDate>
        
        <guid>https://alphacoder.xyz/vehicle-counting/</guid>
        <description>Alpha Coder https://alphacoder.xyz/vehicle-counting/ -&lt;p&gt;I worked on a video-based vehicle counting system (VCS) for my final year (BSc) project. I shared &lt;a href=&#34;https://twitter.com/nicholaskajoh/status/1115016849840844805?s=20&#34;&gt;a demo on Twitter&lt;/a&gt; that went semi-viral!&lt;/p&gt;
&lt;p&gt;In this article, I&amp;rsquo;ll explain why and take you through how I built it, discussing how it works, how I learned the libraries used, the components of the system, the algorithms and models I experimented with and the results obtained. Let&amp;rsquo;s get started!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; I built a video-based vehicle counting system using Python/OpenCV. You can find the &lt;a href=&#34;https://github.com/nicholaskajoh/Vehicle-Counting&#34;&gt;code on my GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/vcs/vehicle_counting.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;how-it-works&#34;&gt;How it works&lt;/h1&gt;
&lt;p&gt;The vehicle counting system I built is made up of three main components: a detector, tracker and counter. The detector identifies vehicles in a given frame of video and returns a list of bounding boxes around the vehicles to the tracker. The tracker uses the bounding boxes to track the vehicles in subsequent frames. The detector is also used to update the trackers periodically to ensure that they are still tracking the vehicles correctly. The counter counts vehicles when they leave the frame or makes use of a counting line drawn across a road.&lt;/p&gt;
&lt;h1 id=&#34;why-vehicle-counting&#34;&gt;Why vehicle counting?&lt;/h1&gt;
&lt;p&gt;Computer Vision (CV) had been on my list of things to learn for a long time so I decided to use the opportunity of my final project to learn it. I actually wanted to build a turn-based or real-time strategy game that used a healthy dose of AI but I knew I wouldn&amp;rsquo;t have been able to complete it in time for my defence so I figured a CV project was the way to go as I&amp;rsquo;d very likely get to use Machine Learning (ML).&lt;/p&gt;
&lt;p&gt;Computer Vision is an interdisciplinary field concerned with giving computers the ability to &amp;ldquo;see&amp;rdquo; or be able to understand the contents of digital images such as photos and videos. While vision is a trivial task for humans and animals, it&amp;rsquo;s currently quite difficult for machines. However, a lot of progress has been made in the field in the last few decades and new techniques and technologies to make CV faster and more accurate are actively being researched.&lt;/p&gt;
&lt;p&gt;A vehicle counting system, as you might have already inferred, is a system that counts vehicles on the road. Why would you want to build one? Why would you want to count vehicles on the road? Here are some reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Traffic management and planning:&lt;/strong&gt; If you have a good sense of the volume of traffic moving along a given road or network of roads, you can better understand congestion and then manage and/or make plans to reduce/eliminate it. Vehicle count data is very useful to urban city planners and transport authorities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Traffic control:&lt;/strong&gt; No one likes to be stuck behind a red light especially when the road is free. Vehicle counting systems can be integrated with traffic light control software to intelligiently direct vehicles based on the current traffic situation in real time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parking management:&lt;/strong&gt; A VCS can be installed at the entrance of a parking lot to monitor vehicles coming in and going out in order to determine whether there are slots available at any given time. It can also be used to ensure the number of vehicles in a given place (such as a hotel or events center) does not exceed its capacity by controlling Automatic Barrier Gates as opposed to issuing tags.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Advertising:&lt;/strong&gt; Billboard advertisers and their clients are interested in the volume of vehicular traffic along a road where they have ads or where they want to install a billboard because they can make estimates of the number of people who see their ads per time using the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;why-video&#34;&gt;Why video?&lt;/h1&gt;
&lt;p&gt;There are a handful of ways to count vehicles on the road from manual counts to pneumatic tubes to piezoelectric sensors. Why was video used? Why is it preferred?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sensor data (video footage) can be used to verify the system’s results which
makes it easier and faster to evaluate and improve the system.&lt;/li&gt;
&lt;li&gt;The footage can also be used for other purposes including surveillance,
automatic plate number recognition, vehicle type detection and vehicle speed
detection to name a few.&lt;/li&gt;
&lt;li&gt;It is relatively cheaper to implement and scale as a permanent vehicle counting
system compared to other systems.&lt;/li&gt;
&lt;li&gt;It can track and count multiple vehicles moving in different directions across
several lanes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;learning-opencv&#34;&gt;Learning OpenCV&lt;/h1&gt;
&lt;p&gt;Since I&amp;rsquo;m proficient in Python, OpenCV was the logical tool to use. &lt;a href=&#34;https://opencv.org&#34;&gt;OpenCV&lt;/a&gt; is an
open-source library made up of a collection of modules for performing real-time Computer
Vision tasks. I used the YouTube tutorial series &lt;a href=&#34;https://www.youtube.com/playlist?list=PLQVvvaa0QuDdttJXlLtAJxJetJcqmqlQq&#34;&gt;OpenCV with Python for Image and Video Analysis&lt;/a&gt; by Harrison Kinsley of &lt;a href=&#34;https://pythonprogramming.net&#34;&gt;PythonProgramming.net&lt;/a&gt; to learn the basics. I was quickly able to learn how to load images and videos, overlay text, shapes or images on media, manipulate pixels on images as a pre-processing step or to produce a visual, perform background substraction to detect objects, run Haar Cascades for object detection etc.&lt;/p&gt;
&lt;h1 id=&#34;components-of-the-vcs&#34;&gt;Components of the VCS&lt;/h1&gt;
&lt;p&gt;I had little or no idea on how to build a VCS so I looked online for inspiration. I found a C++ project which used background subtraction and decided to port it to Python. This gave me to opportunity to go through every line of the code and understand it at a deeper level. I completed the port and tested it out on the traffic scenes I&amp;rsquo;d recorded. The results were very poor. I began thinking of better algorithms and techniques to use. I broke the problem down into three sub-problems: detection, tracking and counting.&lt;/p&gt;
&lt;h2 id=&#34;detection&#34;&gt;Detection&lt;/h2&gt;
&lt;p&gt;This is a crucial and probably the most important part of building a VCS. Detection is an aspect of CV and image processing concerned with identifying instances of objects of a certain class, like vehicles or people, in images and videos. Popular areas of interest in object detection include pedestrian detection and face detection. Object detection can be applied in solving hard problems in areas like image search and video surveillance. It is used widely in computer vision tasks including face detection, face recognition, and object tracking.&lt;/p&gt;
&lt;p&gt;All object types have special attributes that help in classifying them. For instance, all faces are round. Object detection algorithms use these special attributes to identify
objects in images and videos. I experimented with several object detection techniques, most notably background subtraction, Haar Cascades and YOLO.&lt;/p&gt;
&lt;h3 id=&#34;background-subtraction&#34;&gt;Background subtraction&lt;/h3&gt;
&lt;p&gt;The first detector I used to id vehicles was a background substractor. Background or image subtraction is the process of extracting the foreground of an image from its background. If you have a background image like a road without vehicles in it, you can subtract this image from another image of the same road (from the exact same view) which contains vehicles to detect those vehicles. The background pixels would cancel each other out and the objects in the foreground would pop out.&lt;/p&gt;
&lt;p&gt;What if you don&amp;rsquo;t have a background image? You can achieve the same results if the objects you&amp;rsquo;re interested in detecting are in motion, the background is static and the camera is stationary. In this case, all you have to do is evaluate the difference between consecutive images i.e image 2 minus image 1, image 3 minus image 2, image 4 minus image 3 etc. This works because the pixels of moving objects constantly shift hence are not cancelled out like the background pixels and thus pop out as the foreground.&lt;/p&gt;
&lt;p&gt;The images below show what background substraction looks like.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/vcs/frame.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/vcs/bgsub.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;While background substraction was easy to implement and super fast, I was unable to arrive at a suitable threshold for detecting vehicle blobs. Vehicles are not the only objects that move on/across the road. There are pedestrians, animals, people with carts or wheelbarrows, skateboarders etc. Also, objects change in size and shape as they move across the view due to perspective, and vehicles may be occluded by other vehicles in the scene and thus interpreted as one object.&lt;/p&gt;
&lt;p&gt;I observed a lot of noise as well mainly due to the waving of trees and commercial activities going on in the background. While noise reduction techniques such as Gaussian Blur and selecting a Region of Interest (ROI) for detection helped, the overall performance of the detector was still underwhelming. After spending quite a while trying to tweak things in order to increase accuracy, I decided I needed another detector.&lt;/p&gt;
&lt;h3 id=&#34;haar-cascades&#34;&gt;Haar Cascades&lt;/h3&gt;
&lt;p&gt;Haar Cascades are object detection models based on the concept of &lt;a href=&#34;https://en.wikipedia.org/wiki/Haar-like_feature&#34;&gt;Haar-like features&lt;/a&gt; developed by Paul Viola and Michael Jones and published in their 2001 paper titled &lt;a href=&#34;https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf&#34;&gt;Rapid Object Detection using a Boosted Cascade of Simple Features&lt;/a&gt;. I tried out a car detector Haar Cascade I found online. It produced a lot of false positives which made counts erroneous. I considered creating my own model but it seemed like a Herculean task at the time. I decided to try deep learning instead.&lt;/p&gt;
&lt;p&gt;I plan to create a custom Haar Cascade with data got from testing the VCS in the real world in the near future. Haar Cascades are generally a lot faster than deep learning object detection models so if I can get the model to be just as accurate or more accurate than the deep learning alternatives, it might be possible to run the VCS efficiently on devices like Raspberry Pis instead of depending on the cloud for processing power.&lt;/p&gt;
&lt;p&gt;If you want to learn more about Haar Cascades, &lt;a href=&#34;https://www.youtube.com/watch?v=uEJ71VlUmMQ&#34;&gt;check out this video&lt;/a&gt; which explains how the technique works for face detection.&lt;/p&gt;
&lt;h3 id=&#34;yolo&#34;&gt;YOLO&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://pjreddie.com/darknet/yolo/&#34;&gt;YOLO&lt;/a&gt; (You Only Look Once) is a popular deep learning model/architecture for object detection. I found out about it through &lt;a href=&#34;https://www.youtube.com/watch?v=Cgxsv1riJhI&#34;&gt;a TED talk given by one of its creators Joseph Redmon&lt;/a&gt; and wondered if I could use it in the VCS project. Fortunately, OpenCV provides a deep neural network module with which can be used to import and run YOLO models easily. I downloaded a model trained on the &lt;a href=&#34;http://cocodataset.org&#34;&gt;COCO dataset&lt;/a&gt; from the YOLO website and tried it out. The results were very good so I stuck with YOLO for detection. I&amp;rsquo;ve since added support for other deep learning models/libraries. It&amp;rsquo;s actually quite easy to modify the code to support a model of your choice.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/vcs/yolo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;If you want to learn more about YOLO, you should definitely take a look at these papers: &lt;a href=&#34;https://pjreddie.com/media/files/papers/yolo_1.pdf&#34;&gt;You Only Look Once:
Unified, Real-Time Object Detection&lt;/a&gt; and &lt;a href=&#34;https://pjreddie.com/media/files/papers/YOLOv3.pdf&#34;&gt;YOLOv3: An Incremental Improvement&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;tracking&#34;&gt;Tracking&lt;/h2&gt;
&lt;p&gt;Tracking is the process of following the path or movements of an object with the purpose of finding it or observing its course. The uses of video tracking include augmented reality, surveillance and security, video compression and communication, video editing, human-computer interaction, traffic control and medical imaging.&lt;/p&gt;
&lt;p&gt;The goal of tracking is to associate target objects in sequential frames of a video. This association can be very hard to accomplish when the objects are moving fast in relation to the frame rate of the video. Things get even more complicated when tracked objects change their orientation over time. In this scenario, video tracking systems normally use a motion model which details how the image of the target might look for several possible orientations of the object.&lt;/p&gt;
&lt;h3 id=&#34;centroid-tracking&#34;&gt;Centroid tracking&lt;/h3&gt;
&lt;p&gt;I first experimented with the centroid tracking algorithm since it was easy for me to implement. Centroid tracking works by associating the centroid of a bounding box around a detected object in one frame with a centroid in a subsequent frame based on some measure of proximity between them such as euclidean distance. Aside having to run detection on every frame which is computationally expensive, it was difficult for me to find a good threshold that reliably determines if two consecutive centroids belong to a single object or not. Among other things, the value for this threshold must consider the size of the object, and the frame rate and resolution of the video. After several failed attempts to tweak the algorithm to work well with my test videos, I started looking for alternatives.&lt;/p&gt;
&lt;h3 id=&#34;opencv-tracking-algorithms-csrt-and-kcf&#34;&gt;OpenCV tracking algorithms (CSRT and KCF)&lt;/h3&gt;
&lt;p&gt;In my search for better tracking algorithms, I found out that &lt;a href=&#34;https://www.learnopencv.com/object-tracking-using-opencv-cpp-python/&#34;&gt;OpenCV has implementations of some state-of-the-art algorithms&lt;/a&gt; including BOOSTING, MIL, KCF, TLD, MEDIANFLOW, GOTURN, MOSSE and CSRT. I settled for CSRT (&lt;a href=&#34;https://pdfs.semanticscholar.org/b16a/583ee173f222c690242aaff7925838893fe8.pdf&#34;&gt;Discriminative Correlation Filter with Channel and Spatial Reliability&lt;/a&gt;) and KCF (&lt;a href=&#34;https://arxiv.org/pdf/1404.7584.pdf&#34;&gt;Kernelized Correlation Filters&lt;/a&gt;) because they gave the best results. CSRT, while more accurate, is computationally expensive. In traffic scenes where there are not a lot of vehicles on the road, KCF usually does better.&lt;/p&gt;
&lt;p&gt;There was a challenge with CSRT AND KCF however. Vehicles, and thus their bounding boxes, change in size as they move across the frame of a video due to perspective but these trackers can&amp;rsquo;t adjust the size of their bounding boxes as they track an object. Eventually when an object is large or small enough, the algorithm becomes unable to find the object even though it&amp;rsquo;s still in frame. To solve this problem, the detector is periodically run to update the trackers with new bounding boxes. But before going for this approach, I experimented with Camshift.&lt;/p&gt;
&lt;h3 id=&#34;camshift&#34;&gt;Camshift&lt;/h3&gt;
&lt;p&gt;CAMshift (Continuously Adaptive Meanshift) is a tracking algorithm developed by Gary Bradski and described in his 1988 paper titled &lt;a href=&#34;http://opencv.jp/opencv-1.0.0_org/docs/papers/camshift.pdf&#34;&gt;Computer Vision Face Tracking for Use in a Perceptual User Interface&lt;/a&gt;. Unlike CSRT and KCF, it adapts a bounding box with the size and rotation of its target object. However, I observed a problem which made it unusable for the VCS. Camshift was having a hard time tracking vehicles because they were moving too fast for it. It performed pretty well when I tried to track slow-moving objects but was always left behind by fast-moving ones (e.g vehicles) shortly after the tracking began.&lt;/p&gt;
&lt;h2 id=&#34;counting&#34;&gt;Counting&lt;/h2&gt;
&lt;p&gt;Counting was the easiest part! Vehicles are counted when they leave the frame or cross a line at an exit point of the frame. Using a counting line makes it easier to count vehicles moving in a certain direction.&lt;/p&gt;
&lt;h1 id=&#34;contribute&#34;&gt;Contribute&lt;/h1&gt;
&lt;p&gt;The VCS is free and open source software available &lt;a href=&#34;https://github.com/nicholaskajoh/Vehicle-Counting&#34;&gt;on GitHub&lt;/a&gt;. You can contribute to the project by taking it for a spin and reporting issues/bugs or working on parts of it that need improvement.&lt;/p&gt;
- https://alphacoder.xyz/vehicle-counting/ - Copyright 2020 to ∞. Try Catch Finally Solutions Ltd. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>Machine Learning explained</title>
        <link>https://alphacoder.xyz/machine-learning-explained/</link>
        <pubDate>Sat, 18 May 2019 15:22:12 +0000</pubDate>
        
        <guid>https://alphacoder.xyz/machine-learning-explained/</guid>
        <description>Alpha Coder https://alphacoder.xyz/machine-learning-explained/ -&lt;p&gt;Over the past couple of weeks, I got to interact with quite a number of people who wanted to build AI (Artificial Intelligience) projects using Machine Learning (ML). There was one recurring problem I noticed in my discussions with them — they didn&amp;rsquo;t actually understand what Machine Learning is. And without understanding — at least on a high level — it&amp;rsquo;s nearly impossible to develop anything worth while. Except of course you intend to download AI projects off GitHub. Even then, you might have a hard time getting them to work or customizing them to suit your needs.&lt;/p&gt;
&lt;p&gt;In this article, I&amp;rsquo;ll be explaining what Machine Learning is with newbies in mind. I&amp;rsquo;ll also expatiate on some of the commonly used jargon in the space so that you can more easily find your way around ML material and improve your skills.&lt;/p&gt;
&lt;h1 id=&#34;what-is-machine-learning&#34;&gt;What is Machine Learning?&lt;/h1&gt;
&lt;p&gt;ML is an approach to building Artificial Intelligence (AI) systems which involves writing computer programs that &amp;ldquo;learn&amp;rdquo; from data. That is, they are not explicitly programmed to perform a specific task such as solving a quadratic equation or authenticating a user. Instead, they learn to solve a given problem by example.&lt;/p&gt;
&lt;p&gt;AI is an aspect of computer science concerned with building intelligent machines or software. Intelligence is a rather vague concept (especially in today&amp;rsquo;s world of tech where just about everything is &amp;ldquo;smart&amp;rdquo;) so this definition is probably not sufficient.&lt;/p&gt;
&lt;p&gt;In simple terms, intelligence in AI is the ability of computers to solve problems that are easier for humans or animals to do e.g driving a car or searching for food. There are several techniques used in AI today but our focus here is ML as you&amp;rsquo;d expect.&lt;/p&gt;
&lt;p&gt;Say you wanted to write a program that can determine if a photo contains a dog or not. How would you go about it?&lt;/p&gt;
&lt;p&gt;You could try to code features that may be used to identify a dog as well as what combinations of these features distinguish dogs from other animals (and objects). That&amp;rsquo;s a hard enough task. But you&amp;rsquo;ll also have to consider:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Position:&lt;/strong&gt; Is the dog sitting or standing or lying down or running? What part of the photo is the dog located?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Size and orientation:&lt;/strong&gt; How large or small is the dog in comparison to the rest of the photo? Is the photo rotated?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Occlusion:&lt;/strong&gt; Is any part of the dog covered by another object? What part? By how much?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s say you succeeded in creating the perfect dog detector. What happens when you want to detect ducks. Can you repurpose the dog detector to also detect ducks. Probably not, because you&amp;rsquo;ll be dealing with an almost completely new set of features and combinations.&lt;/p&gt;
&lt;p&gt;Fortunately, ML excels in these sort of tasks because we can show an ML algorithm photos of dogs and have it figure out the necessary features and combinations, as well as consider position, size, orientation, and occlusion. More so, it can be repurposed to detect ducks, cats and indeed any other animal or object for that matter just by showing it relevant photos. We&amp;rsquo;ll find out a little bit about how this can be done as we proceed.&lt;/p&gt;
&lt;h1 id=&#34;what-are-ml-models&#34;&gt;What are ML models?&lt;/h1&gt;
&lt;p&gt;A machine learning model is essentially a function which maps a set of inputs to a set of outputs. The inputs are features (e.g of a human being — height, weight etc) and the outputs are predictions (e.g sex — male or female).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take the following mathematical function for example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;V = 4/3 * PI * r^3
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This is the formula for the volume of a sphere. The input is &lt;code&gt;r&lt;/code&gt; (the radius of the sphere) and the output is &lt;code&gt;V&lt;/code&gt; (the sphere&amp;rsquo;s volume). &lt;a href=&#34;https://en.wikipedia.org/wiki/Pi&#34;&gt;Pi&lt;/a&gt; is not an input since it&amp;rsquo;s constant (about 3.14), as is the literal value &lt;code&gt;4/3&lt;/code&gt;. The sphere volume formula was derived analytically using tools like calculus, geometry and trigonometry. Not all problems can be solved through such means. Some problems are not feasible or too complex to model in this way. It&amp;rsquo;s a lot easier to show computers the problem and/or answers and have them figure out an approximate model that solves it.&lt;/p&gt;
&lt;h3 id=&#34;training&#34;&gt;Training&lt;/h3&gt;
&lt;p&gt;Models are optimized or fine-tuned through a process known as training. Say you wanted to build your dog detector, like we considered in a previous section, by creating a machine learning model. At the point of initialization, the dog detector model is like a newborn baby. It doesn&amp;rsquo;t know anything and behaves rather randomly. During training, it&amp;rsquo;s shown a bunch of photos of dogs of different types, shapes and sizes, as well as photos of other objects so that it can make the right distinctions. At the end of training, you&amp;rsquo;d expect it to know how to detect dogs with a high level of accuracy.&lt;/p&gt;
&lt;h3 id=&#34;datasets&#34;&gt;Datasets&lt;/h3&gt;
&lt;p&gt;Data is the heart and soul of machine learning — it&amp;rsquo;s what &amp;ldquo;machines&amp;rdquo; use to &amp;ldquo;learn&amp;rdquo;. A dataset is a collection of data which can be used to train, test and validate machine learning models. The more varied and rich a dataset is, the better a model we can produce. Good data directly translates to a good model. As such, it&amp;rsquo;s important to know what good data for a given problem looks like and how to obtain it. Machine learning models, unlike humans need a lot of data to work well. Some models consume millions of records! Knowing where to source data as well as how to prepare it for training is an invaluable skill for any ML practitioner to hone.&lt;/p&gt;
&lt;h3 id=&#34;testing&#34;&gt;Testing&lt;/h3&gt;
&lt;p&gt;Testing is the process of evaluating a model to see how well it performs. It helps us know how well a model is doing and where it needs work, as well as benchmark it against other models. Datasets are usually divided into training and testing sets. Models are trained with one set, and tested and validated with others so that we can be sure that they&amp;rsquo;re actually learning instead of overfitting. If your math teacher brings the examples they gave in class in an exam, you don&amp;rsquo;t need to understand the calculations to pass. You can just cram your notes and ace the test without understanding a thing. This is bad because when faced with similar problems in an external exam or competition, you&amp;rsquo;ll likely perform poorly. The same goes for an overfitted model.&lt;/p&gt;
&lt;p&gt;There are several metrics used in evaluating ML models. Some of the more popular ones include accuracy, confidence, &lt;a href=&#34;https://en.wikipedia.org/wiki/Confusion_matrix&#34;&gt;confusion matrices&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Mean_squared_error&#34;&gt;mean squared error&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/F1_score&#34;&gt;F1 score&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;types-of-ml&#34;&gt;Types of ML&lt;/h1&gt;
&lt;p&gt;ML algorithms are generally classified into 3 main types. They are:&lt;/p&gt;
&lt;h3 id=&#34;supervised-learning&#34;&gt;Supervised learning&lt;/h3&gt;
&lt;p&gt;This is a type of machine learning which involves the use of labelled data. That is, we have a dataset containing input features (e.g the heights and weights of a number of people) and output labels (e.g their sex or age), and we want an ML algorithm to be able to make accurate predictions after learning from such data. Supervised learning is divided into classification and regression. Classification models output discrete values e.g &amp;ldquo;male&amp;rdquo; or &amp;ldquo;female&amp;rdquo;, or &amp;ldquo;red&amp;rdquo;, &amp;ldquo;blue&amp;rdquo;, or &amp;ldquo;green&amp;rdquo;, while regression models output continuous values such as a prediction for the price of a barrel of crude oil next year e.g $50.43.&lt;/p&gt;
&lt;h3 id=&#34;unsupervised-learning&#34;&gt;Unsupervised learning&lt;/h3&gt;
&lt;p&gt;Unsupervised learning involves the use of unlabelled data to build machine learning models. We basically give an algorithm data and let it figure out patterns in it. We might give an ML algorithm the weights, heights and other features of individuals in a school and it might group them into &amp;ldquo;healthy&amp;rdquo; or &amp;ldquo;sick&amp;rdquo;, or &amp;ldquo;plays football&amp;rdquo;, &amp;ldquo;plays basketball&amp;rdquo; or &amp;ldquo;plays volleyball&amp;rdquo;. This sort of clustering provides valuable insights that can find application in a number of areas such as recommendation and segmentation.&lt;/p&gt;
&lt;h3 id=&#34;reinforcement-learning&#34;&gt;Reinforcement learning&lt;/h3&gt;
&lt;p&gt;Reinforcement learning involves building goal-oriented models that learn through &amp;ldquo;rewards&amp;rdquo; and &amp;ldquo;punishment&amp;rdquo;. When an agent trained using reinforcement learning reaches a desired state, it is incentivized with a reward and if it reaches an undesired state, it is discouraged through punishment. This technique was used to build &lt;a href=&#34;https://deepmind.com/blog/alphago-zero-learning-scratch/&#34;&gt;AlphaGo Zero&lt;/a&gt;, an AI agent which was able to beat the world&amp;rsquo;s best &lt;a href=&#34;https://en.wikipedia.org/wiki/Go_(game)&#34;&gt;Go&lt;/a&gt; players.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NB:&lt;/strong&gt; I have a blog series here on Alpha Coder called &lt;a href=&#34;https://alphacoder.xyz/tag/ml-chops-series&#34;&gt;ML Chops&lt;/a&gt;. It&amp;rsquo;s a set of tutorials on how some popular ML algorithms work and how to implement them from scratch using the Python programming language. You should check it out!&lt;/p&gt;
&lt;h1 id=&#34;neural-networks-and-deep-learning&#34;&gt;Neural networks and deep learning&lt;/h1&gt;
&lt;p&gt;If you follow tech and startup news, you&amp;rsquo;ve probably heard the words &amp;ldquo;neural networks&amp;rdquo; and &amp;ldquo;deep learning&amp;rdquo; being tossed all around. What are they? And why is there so much hype around them?&lt;/p&gt;
&lt;p&gt;A neural network is a machine learning framework made up of a collection of connected nodes called neurons arranged in layers. A neuron receives input signals from one or more neurons behind it, performs one or more computations with them and transmits an output signal to the neurons in front of it. The nodes at the ends of the network are the input and output layers while the nodes inbetween form sets of hidden layers. Neural networks are loosely modelled after biological neural networks which allow living things perform very complex physical and chemical activities. If you want to learn more about neural networks, &lt;a href=&#34;https://www.youtube.com/watch?v=aircAruvnKk&#34;&gt;But what *is* a Neural Network?&lt;/a&gt; by &lt;em&gt;3Blue1Brown&lt;/em&gt; is one of the best resources out there to give you a comprehensive introduction.&lt;/p&gt;
&lt;p&gt;Deep learning is the creation and training of neural networks that contain more than one hidden layer. A network with one hidden layer is a regular neural network and one with two or more hidden layers is a &amp;ldquo;deep&amp;rdquo; neural network. Deep neural networks usually perform better than regular ones because they can encode more information within the network. However, using deeper neural networks doesn&amp;rsquo;t necessarily translate to obtaining better results. There are other factors involved.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s a lot of hype around neural networks and deep learning because these tools have changed the game by making feats that seemed impossible just a couple years ago possible. They are used heavily in building state-of-the-art Computer Vision and Natural Language Processing (NLP) applications today.&lt;/p&gt;
&lt;h1 id=&#34;ml-is-not-a-silver-bullet&#34;&gt;ML is not a silver bullet&lt;/h1&gt;
&lt;p&gt;Analytical models, if they&amp;rsquo;re possible and feasible to arrive at, are better than machine learning because they produce answers and not predictions. ML is not a silver bullet like some may have you believe. There are problems machine learning methods excel in. There are others where analytical approaches are simpler and produce better results. Before employing ML, ask yourself: can this problem be solved analytically?&lt;/p&gt;
- https://alphacoder.xyz/machine-learning-explained/ - Copyright 2020 to ∞. Try Catch Finally Solutions Ltd. All rights reserved.</description>
        </item>
    
    
    
        <item>
        <title>How to configure wildcard subdomains</title>
        <link>https://alphacoder.xyz/how-to-configure-wildcard-subdomains/</link>
        <pubDate>Tue, 02 Apr 2019 04:08:11 +0000</pubDate>
        
        <guid>https://alphacoder.xyz/how-to-configure-wildcard-subdomains/</guid>
        <description>Alpha Coder https://alphacoder.xyz/how-to-configure-wildcard-subdomains/ -&lt;p&gt;Some web apps, especially those for the enterprise, give every organization, team or user their own subdomain such as &lt;strong&gt;team-name.awesomeapp.com&lt;/strong&gt; or &lt;strong&gt;org-name.beta.awesomeapp.com&lt;/strong&gt;. These subdomains are variable, meaning that they are not predefined and can contain any valid &lt;a href=&#34;https://en.wikipedia.org/wiki/Domain_name&#34;&gt;domain name&lt;/a&gt; characters.&lt;/p&gt;
&lt;p&gt;To configure wildcard subdomains, all you need to do is add a &amp;ldquo;match all&amp;rdquo; CNAME record for your domain. The way a given subdomain is handled is totally up to your application. All web browsers and servers provide a medium for accessing a web app&amp;rsquo;s url for a given request, so you can fetch the subdomain and proceed with your business logic. Often, the subdomain would be a unique id for something e.g an organization, so your can use it to get the organization&amp;rsquo;s details for instance.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a sample application that displays avatars from &lt;a href=&#34;http://avatars.adorable.io&#34;&gt;Adorable Avatars&lt;/a&gt; based on the subdomain you use.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/wcsubd/demo.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Visit &lt;code&gt;some-random-name.wildcard-subdomains.alphacoder.xyz&lt;/code&gt; where &lt;code&gt;some-random-name&lt;/code&gt; can be anything you like and watch the avatars change!&lt;/p&gt;
&lt;p&gt;To use wildcard subdomains, go to your domain registrar and create a &amp;ldquo;match all&amp;rdquo; CNAME record&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://alphacoder.xyz/images/wcsubd/dns-conf.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;If you want subdomains like &lt;code&gt;some-random-name.alphacoder.xyz&lt;/code&gt;, feel free to skip the &lt;code&gt;.wildcard-subdomains&lt;/code&gt; so that you have only &lt;code&gt;*&lt;/code&gt; in the host field.&lt;/p&gt;
&lt;p&gt;The web page has just a few lines of code&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html lang=&amp;quot;en&amp;quot;&amp;gt;
&amp;lt;head&amp;gt;
  &amp;lt;title&amp;gt;Wildcard subdomains&amp;lt;/title&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;

  &amp;lt;h1 id=&amp;quot;subdomain&amp;quot;&amp;gt;&amp;lt;/h1&amp;gt;
  &amp;lt;img id=&amp;quot;avatar&amp;quot;&amp;gt;

  &amp;lt;script&amp;gt;
    var subdomain = window.location.hostname.split(&#39;.&#39;)[0];
    document.getElementById(&#39;subdomain&#39;).textContent = subdomain;
    var avatarSrc = &#39;https://api.adorable.io/avatars/285/&#39; + subdomain + &#39;.png&#39;;
    document.getElementById(&#39;avatar&#39;).src = avatarSrc;
  &amp;lt;/script&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most important part of the code is line 12, which gets the subdomain from the hostname (&lt;code&gt;window.location.hostname.split(&#39;.&#39;)[0]&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;In development, you can use &lt;code&gt;lvh.me&lt;/code&gt; (e.g &lt;code&gt;some-random-name.lvh.me:8080&lt;/code&gt;) to test your application instead of &lt;code&gt;127.0.0.1&lt;/code&gt;. &lt;code&gt;some-random-name.localhost&lt;/code&gt; also works on browsers I&amp;rsquo;ve tested.&lt;/p&gt;
- https://alphacoder.xyz/how-to-configure-wildcard-subdomains/ - Copyright 2020 to ∞. Try Catch Finally Solutions Ltd. All rights reserved.</description>
        </item>
    
    
  </channel>
</rss> 